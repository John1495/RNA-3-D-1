{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPICtkV0PLjeSLVZ6ny7Wh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John1495/RNA-3-D-1/blob/main/GVP_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G-76ER7QI-SW",
        "outputId": "ccfb1cd4-351e-4cac-9d41-23a9aaef79d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrFzuY-sDzCv",
        "outputId": "32a78b4a-b35f-4b41-ef6b-7934f164ff79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_scatter-2.1.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.4/500.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_sparse-0.6.18%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_cluster-1.6.3%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (753 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt21cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt21cpu torch-sparse-0.6.18+pt21cpu torch-spline-conv-1.2.2+pt21cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tqdm import tqdm\n",
        "\n",
        "# == Load Data ==\n",
        "seq_df = pd.read_csv('/kaggle/cleaned_train_sequences2 (1).csv')\n",
        "label_df = pd.read_csv('/kaggle/train_labels1.csv')\n",
        "\n",
        "label_df['resname'] = label_df['resname'].str.extract(r'([AUGC])')\n",
        "label_df = label_df.dropna(subset=['resname'])\n",
        "label_df['target_id'] = label_df['ID'].str.extract(r'(.+)_\\d+')\n",
        "\n",
        "merged = pd.merge(label_df, seq_df[['target_id', 'sequence']], on='target_id', how='left')\n",
        "\n",
        "# Filter for complete RNAs\n",
        "valid_ids = merged.groupby('target_id')['resid'].count()\n",
        "valid_ids = valid_ids[valid_ids > 10].index\n",
        "merged = merged[merged['target_id'].isin(valid_ids)]\n",
        "\n",
        "train_ids, val_ids = train_test_split(merged['target_id'].unique(), test_size=0.1, random_state=42)\n",
        "residue_mapping = {'A': 0, 'U': 1, 'G': 2, 'C': 3}\n",
        "\n",
        "# == Graph Creator ==\n",
        "def create_graph(df_group, scaler=None, fit_scaler=False):\n",
        "    df_group = df_group.sort_values('resid')\n",
        "    coords = df_group[['x_1', 'y_1', 'z_1']].values\n",
        "\n",
        "    if scaler:\n",
        "        coords = scaler.fit_transform(coords) if fit_scaler else scaler.transform(coords)\n",
        "\n",
        "    node_scalar = torch.eye(4)[[residue_mapping[r] for r in df_group['resname']]]\n",
        "\n",
        "    # Vector features are placeholder zeros for now (can be enhanced)\n",
        "    node_vector = torch.zeros((len(df_group), 4))\n",
        "\n",
        "    node_features = torch.cat([node_scalar, node_vector], dim=1)\n",
        "\n",
        "    pos = torch.tensor(coords, dtype=torch.float)\n",
        "    y = pos\n",
        "    n = len(df_group)\n",
        "\n",
        "    edge_index = torch.tensor([[i, j] for i in range(n) for j in range(n) if i != j], dtype=torch.long).t().contiguous()\n",
        "    return Data(x=node_features, edge_index=edge_index, pos=pos, y=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_graphs = [create_graph(merged[merged['target_id'] == tid], scaler, True) for tid in tqdm(train_ids)]\n",
        "val_graphs = [create_graph(merged[merged['target_id'] == tid], scaler, False) for tid in tqdm(val_ids)]\n",
        "\n",
        "train_loader = DataLoader(train_graphs, batch_size=1)\n",
        "val_loader = DataLoader(val_graphs, batch_size=1)\n",
        "\n",
        "# == GVP Block ==\n",
        "class GVPBlock(nn.Module):\n",
        "    def __init__(self, scalar_dim, vector_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scalar_mlp = nn.Sequential(\n",
        "            nn.Linear(scalar_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.vector_mlp = nn.Sequential(\n",
        "            nn.Linear(vector_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_scalar, x_vector):\n",
        "        s_out = self.scalar_mlp(x_scalar)\n",
        "        v_out = self.vector_mlp(x_vector)\n",
        "        return s_out, v_out\n",
        "\n",
        "# == Full GVP Model ==\n",
        "class PowerfulGVPModel(nn.Module):\n",
        "    def __init__(self, scalar_dim=4, vector_dim=4, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.gvp1 = GVPBlock(scalar_dim, vector_dim, hidden_dim)\n",
        "        self.gvp2 = GVPBlock(hidden_dim, hidden_dim, hidden_dim)\n",
        "        self.gvp3 = GVPBlock(hidden_dim, hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        x_scalar = x[:, :4]\n",
        "        x_vector = x[:, 4:]\n",
        "\n",
        "        s, v = self.gvp1(x_scalar, x_vector)\n",
        "        s, v = self.gvp2(s, v)\n",
        "        s, v = self.gvp3(s, v)\n",
        "\n",
        "        x_combined = s + v\n",
        "        out = self.fc(x_combined)\n",
        "        return out\n",
        "\n",
        "# == Training ==\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PowerfulGVPModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch)\n",
        "        loss = loss_fn(pred, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}: Train Loss = {avg_loss:.6f}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        no_improve = 0\n",
        "        torch.save(model.state_dict(), \"best_gvp_model.pth\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# == Evaluation ==\n",
        "model.load_state_dict(torch.load(\"best_gvp_model.pth\"))\n",
        "model.eval()\n",
        "predictions, targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        predictions.append(pred.cpu().numpy())\n",
        "        targets.append(batch.y.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions)\n",
        "targets = np.concatenate(targets)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
        "mae = mean_absolute_error(targets, predictions)\n",
        "\n",
        "def calculate_tm_score(true, pred):\n",
        "    d = np.linalg.norm(true - pred, axis=1)\n",
        "    return np.mean(np.exp(-d / (0.5 * len(d))))\n",
        "\n",
        "tm_score = calculate_tm_score(targets, predictions)\n",
        "print(f\"\\nValidation Results:\\nRMSE = {rmse:.4f}, MAE = {mae:.4f}, TM-Score = {tm_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "f2pYsI9JQ3kT",
        "outputId": "9a2a5d23-7853-4041-d964-56851e5eb4f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 747/747 [03:23<00:00,  3.67it/s]\n",
            "100%|██████████| 83/83 [00:30<00:00,  2.69it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss = 0.947850\n",
            "Epoch 1: Train Loss = 0.947782\n",
            "Epoch 2: Train Loss = 0.947724\n",
            "Epoch 3: Train Loss = 0.947745\n",
            "Epoch 4: Train Loss = 0.947692\n",
            "Epoch 5: Train Loss = 0.947705\n",
            "Epoch 6: Train Loss = 0.947678\n",
            "Epoch 7: Train Loss = 0.947668\n",
            "Epoch 8: Train Loss = 0.947661\n",
            "Epoch 9: Train Loss = 0.947675\n",
            "Epoch 10: Train Loss = 0.947646\n",
            "Epoch 11: Train Loss = 0.947617\n",
            "Epoch 12: Train Loss = 0.947606\n",
            "Epoch 13: Train Loss = 0.947662\n",
            "Epoch 14: Train Loss = 0.947625\n",
            "Epoch 15: Train Loss = 0.947601\n",
            "Epoch 16: Train Loss = 0.947591\n",
            "Epoch 17: Train Loss = 0.947593\n",
            "Epoch 18: Train Loss = 0.947586\n",
            "Epoch 19: Train Loss = 0.947578\n",
            "Epoch 20: Train Loss = 0.947577\n",
            "Epoch 21: Train Loss = 0.947572\n",
            "Epoch 22: Train Loss = 0.947568\n",
            "Epoch 23: Train Loss = 0.947601\n",
            "Epoch 24: Train Loss = 0.947578\n",
            "Epoch 25: Train Loss = 0.947569\n",
            "Epoch 26: Train Loss = 0.947565\n",
            "Epoch 27: Train Loss = 0.947565\n",
            "Epoch 28: Train Loss = 0.947565\n",
            "Epoch 29: Train Loss = 0.947567\n",
            "Epoch 30: Train Loss = 0.947559\n",
            "Epoch 31: Train Loss = 0.947558\n",
            "Epoch 32: Train Loss = 0.947561\n",
            "Epoch 33: Train Loss = 0.947560\n",
            "Epoch 34: Train Loss = 0.947556\n",
            "Epoch 35: Train Loss = 0.947556\n",
            "Epoch 36: Train Loss = 0.947558\n",
            "Epoch 37: Train Loss = 0.947557\n",
            "Epoch 38: Train Loss = 0.947555\n",
            "Epoch 39: Train Loss = 0.947554\n",
            "Epoch 40: Train Loss = 0.947556\n",
            "Epoch 41: Train Loss = 0.947555\n",
            "Epoch 42: Train Loss = 0.947553\n",
            "Epoch 43: Train Loss = 0.947554\n",
            "Epoch 44: Train Loss = 0.947553\n",
            "Epoch 45: Train Loss = 0.947573\n",
            "Epoch 46: Train Loss = 0.947551\n",
            "Epoch 47: Train Loss = 0.947547\n",
            "Epoch 48: Train Loss = 0.947620\n",
            "Epoch 49: Train Loss = 0.947567\n",
            "Epoch 50: Train Loss = 0.947559\n",
            "Epoch 51: Train Loss = 0.947552\n",
            "Epoch 52: Train Loss = 0.947554\n",
            "Epoch 53: Train Loss = 0.947547\n",
            "Epoch 54: Train Loss = 0.947546\n",
            "Epoch 55: Train Loss = 0.947555\n",
            "Epoch 56: Train Loss = 0.947546\n",
            "Epoch 57: Train Loss = 0.947542\n",
            "Epoch 58: Train Loss = 0.947545\n",
            "Epoch 59: Train Loss = 0.947545\n",
            "Epoch 60: Train Loss = 0.947544\n",
            "Epoch 61: Train Loss = 0.947539\n",
            "Epoch 62: Train Loss = 0.947541\n",
            "Epoch 63: Train Loss = 0.947546\n",
            "Epoch 64: Train Loss = 0.947541\n",
            "Epoch 65: Train Loss = 0.947540\n",
            "Epoch 66: Train Loss = 0.947538\n",
            "Epoch 67: Train Loss = 0.947539\n",
            "Epoch 68: Train Loss = 0.947541\n",
            "Epoch 69: Train Loss = 0.947538\n",
            "Epoch 70: Train Loss = 0.947536\n",
            "Epoch 71: Train Loss = 0.947537\n",
            "Epoch 72: Train Loss = 0.947540\n",
            "Epoch 73: Train Loss = 0.947537\n",
            "Epoch 74: Train Loss = 0.947535\n",
            "Epoch 75: Train Loss = 0.947536\n",
            "Epoch 76: Train Loss = 0.947539\n",
            "Epoch 77: Train Loss = 0.947536\n",
            "Epoch 78: Train Loss = 0.947535\n",
            "Epoch 79: Train Loss = 0.947534\n",
            "Epoch 80: Train Loss = 0.947537\n",
            "Epoch 81: Train Loss = 0.947535\n",
            "Epoch 82: Train Loss = 0.947535\n",
            "Epoch 83: Train Loss = 0.947545\n",
            "Epoch 84: Train Loss = 0.947535\n",
            "Epoch 85: Train Loss = 0.947532\n",
            "Epoch 86: Train Loss = 0.947532\n",
            "Epoch 87: Train Loss = 0.947534\n",
            "Epoch 88: Train Loss = 0.947535\n",
            "Epoch 89: Train Loss = 0.947533\n",
            "Epoch 90: Train Loss = 0.947534\n",
            "Epoch 91: Train Loss = 0.947534\n",
            "Epoch 92: Train Loss = 0.947564\n",
            "Epoch 93: Train Loss = 0.947533\n",
            "Epoch 94: Train Loss = 0.947529\n",
            "Epoch 95: Train Loss = 0.947531\n",
            "Epoch 96: Train Loss = 0.947534\n",
            "Epoch 97: Train Loss = 0.947529\n",
            "Epoch 98: Train Loss = 0.947532\n",
            "Epoch 99: Train Loss = 0.947531\n",
            "\n",
            "Validation Results:\n",
            "RMSE = 27.4652, MAE = 21.3270, TM-Score = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.model_selection import KFold\n",
        "import copy\n",
        "import time\n",
        "\n",
        "class RNAGraphDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences_file, labels_file, validation_mode=False):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.validation_mode = validation_mode\n",
        "        self.graphs = self._process_data()\n",
        "        self.lengths = [len(seq) for seq in self.sequences['sequence']]\n",
        "        self.max_length = max(self.lengths) if self.lengths else 1\n",
        "\n",
        "    def _process_data(self):\n",
        "        graphs = []\n",
        "        all_coords = []\n",
        "\n",
        "        if not self.validation_mode:\n",
        "            for _, row in self.sequences.iterrows():\n",
        "                target_id = row['target_id']\n",
        "                matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "                if not matched_labels.empty and len(matched_labels) == len(row['sequence']):\n",
        "                    coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "                    all_coords.append(coords)\n",
        "\n",
        "            all_coords = np.concatenate(all_coords, axis=0) if all_coords else np.zeros((1,3))\n",
        "            self.mean = all_coords.mean(axis=0)\n",
        "            self.std = all_coords.std(axis=0) + 1e-8\n",
        "        else:\n",
        "            self.mean = np.zeros(3)\n",
        "            self.std = np.ones(3)\n",
        "\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "\n",
        "            node_feats = self._encode_sequence(sequence)\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "\n",
        "            edge_index, edge_attr = self._build_edges(len(sequence), coords)\n",
        "\n",
        "            graphs.append(Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float32),\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                y=torch.tensor(coords, dtype=torch.float32),\n",
        "                pos=torch.tensor(coords, dtype=torch.float32),\n",
        "                length=torch.tensor(len(sequence), dtype=torch.long)\n",
        "            ))\n",
        "        return graphs\n",
        "\n",
        "    def _encode_sequence(self, sequence):\n",
        "        mapping = {\n",
        "            'A': [1,0,0,0, 0.12, 0.89],\n",
        "            'U': [0,1,0,0, 0.23, 0.76],\n",
        "            'G': [0,0,1,0, 0.34, 0.65],\n",
        "            'C': [0,0,0,1, 0.45, 0.54]\n",
        "        }\n",
        "        return [mapping.get(nt, [0]*6) for nt in sequence]\n",
        "\n",
        "    def _build_edges(self, length, coords):\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        for i in range(length - 1):\n",
        "            vec = coords[i+1] - coords[i]\n",
        "            dist = np.linalg.norm(vec)\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "            edge_attr.extend([\n",
        "                [dist, vec[0], vec[1], vec[2], 1.0],\n",
        "                [dist, -vec[0], -vec[1], -vec[2], 0.0]\n",
        "            ])\n",
        "        return (torch.tensor(edge_index).t().contiguous(),\n",
        "                torch.tensor(np.array(edge_attr), dtype=torch.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.graphs[idx]\n",
        "\n",
        "        if not self.validation_mode and torch.rand(1) < 0.5:\n",
        "            noise_level = 0.02 * (self.lengths[idx]/self.max_length)\n",
        "            data.y += torch.randn_like(data.y) * noise_level\n",
        "            data.pos += torch.randn_like(data.pos) * noise_level\n",
        "            if hasattr(data, 'edge_attr'):\n",
        "                data.edge_attr[:, :4] += torch.randn_like(data.edge_attr[:, :4]) * (noise_level/2)\n",
        "\n",
        "        return data\n",
        "\n",
        "def compute_tm_score(pred_coords, true_coords, lengths):\n",
        "    tm_scores = []\n",
        "    pred_coords = pred_coords.detach().cpu().numpy()\n",
        "    true_coords = true_coords.detach().cpu().numpy()\n",
        "\n",
        "    ptr = 0\n",
        "    for L in lengths:\n",
        "        if isinstance(L, torch.Tensor):\n",
        "            L = L.item()\n",
        "\n",
        "        pred = pred_coords[ptr:ptr+L]\n",
        "        true = true_coords[ptr:ptr+L]\n",
        "        ptr += L\n",
        "\n",
        "        d0 = max(1.24 * (L - 15) ** (1/3) - 1.8, 0.5)\n",
        "        diff = pred - true\n",
        "        dist_sq = np.sum(diff**2, axis=1)\n",
        "        tm_components = 1 / (1 + (dist_sq / (d0**2)))\n",
        "        tm_scores.append(np.sum(tm_components) / L)\n",
        "\n",
        "    return np.mean(tm_scores) if tm_scores else 0.0\n",
        "\n",
        "class RNAD33(nn.Module):\n",
        "    def __init__(self, in_channels=6, hidden_channels=512, num_layers=6, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.LayerNorm(hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = GCNConv(hidden_channels, hidden_channels, improved=True)\n",
        "            bn = nn.BatchNorm1d(hidden_channels)\n",
        "            self.convs.append(conv)\n",
        "            self.bns.append(bn)\n",
        "\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels*2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_channels*2, hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(hidden_channels, 3)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, GCNConv):\n",
        "                nn.init.kaiming_normal_(m.lin.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.input_proj(x)\n",
        "        x_skip = x.clone()\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = conv(x, edge_index) + x\n",
        "            x = bn(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        return self.output_net(x + x_skip)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    tm_scores = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            loss = F.mse_loss(pred, data.y)\n",
        "            total_loss += loss.item()\n",
        "            batch_tm = compute_tm_score(pred, data.y, data.length)\n",
        "            tm_scores.append(batch_tm)\n",
        "\n",
        "    return total_loss / len(loader), np.mean(tm_scores) if tm_scores else 0.0\n",
        "\n",
        "def kfold_validation(train_dataset, config, device):\n",
        "    kfold = KFold(n_splits=config[\"k_folds\"], shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f\"\\n=== Fold {fold + 1}/{config['k_folds']} ===\")\n",
        "\n",
        "        train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_subset,\n",
        "            batch_size=config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: Batch.from_data_list(x),\n",
        "            num_workers=4 if str(device) == 'cuda' else 0\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_subset,\n",
        "            batch_size=config[\"batch_size\"],\n",
        "            collate_fn=lambda x: Batch.from_data_list(x),\n",
        "            num_workers=4 if str(device) == 'cuda' else 0\n",
        "        )\n",
        "\n",
        "        model = RNAD33(\n",
        "            hidden_channels=config[\"hidden_dim\"],\n",
        "            num_layers=config[\"num_layers\"],\n",
        "            dropout=config[\"dropout\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config[\"lr\"],\n",
        "            weight_decay=config[\"weight_decay\"]\n",
        "        )\n",
        "\n",
        "        scheduler = OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=config[\"max_lr\"],\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            epochs=config[\"max_epochs\"],\n",
        "            pct_start=0.3\n",
        "        )\n",
        "\n",
        "        best_val = float('inf')\n",
        "        for epoch in range(1, config[\"max_epochs\"] + 1):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "\n",
        "            for data in train_loader:\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                pred = model(data)\n",
        "                loss = F.mse_loss(pred, data.y)\n",
        "                loss += config[\"l2_lambda\"] * sum(p.norm(2) for p in model.parameters())\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            val_loss, val_tm = validate(model, val_loader, device)\n",
        "\n",
        "            if val_loss < best_val:\n",
        "                best_val = val_loss\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print(f\"Epoch {epoch:03d} | Train Loss: {train_loss/len(train_loader):.6f} | \"\n",
        "                  f\"Val Loss: {val_loss:.6f} | TM-score: {val_tm:.4f}\")\n",
        "\n",
        "        fold_results.append((best_val, best_model))\n",
        "        print(f\"Fold {fold + 1} completed. Best Val Loss: {best_val:.6f}\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "def main():\n",
        "    config = {\n",
        "        \"train_sequences\": \"/kaggle/cleaned_train_sequences2 (1).csv\",\n",
        "        \"train_labels\": \"/kaggle/train_labels1.csv\",\n",
        "        \"validation_sequences\": \"/kaggle/validation_sequences.csv\",\n",
        "        \"validation_labels\": \"/kaggle/validation_labels.csv\",\n",
        "        \"batch_size\": 32 if torch.cuda.is_available() else 8,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"num_layers\": 6,\n",
        "        \"max_epochs\": 200,\n",
        "        \"patience\": 30,\n",
        "        \"min_delta\": 0.0001,\n",
        "        \"grad_clip\": 0.3,\n",
        "        \"lr\": 5e-4,\n",
        "        \"max_lr\": 3e-4,\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"l2_lambda\": 0.01,\n",
        "        \"dropout\": 0.2,\n",
        "        \"k_folds\": 5\n",
        "    }\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading full training dataset...\")\n",
        "    full_train_dataset = RNAGraphDataset(config[\"train_sequences\"], config[\"train_labels\"], validation_mode=False)\n",
        "    print(f\"Loaded {len(full_train_dataset)} training samples\")\n",
        "\n",
        "    print(\"\\nStarting k-fold cross-validation...\")\n",
        "    fold_results = kfold_validation(full_train_dataset, config, device)\n",
        "\n",
        "    avg_val_loss = np.mean([res[0] for res in fold_results])\n",
        "    print(f\"\\nK-fold validation complete. Average Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    print(\"\\nTraining final model on full dataset...\")\n",
        "    train_loader = DataLoader(\n",
        "        full_train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0\n",
        "    )\n",
        "\n",
        "    val_dataset = RNAGraphDataset(\n",
        "        config[\"validation_sequences\"],\n",
        "        config[\"validation_labels\"],\n",
        "        validation_mode=True\n",
        "    )\n",
        "    val_dataset.mean = full_train_dataset.mean\n",
        "    val_dataset.std = full_train_dataset.std\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0\n",
        "    )\n",
        "\n",
        "    model = RNAD33(\n",
        "        hidden_channels=config[\"hidden_dim\"],\n",
        "        num_layers=config[\"num_layers\"],\n",
        "        dropout=config[\"dropout\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config[\"lr\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=config[\"max_lr\"],\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=config[\"max_epochs\"],\n",
        "        pct_start=0.3\n",
        "    )\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_tm = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(1, config[\"max_epochs\"] + 1):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(data)\n",
        "            loss = F.mse_loss(pred, data.y)\n",
        "            loss += config[\"l2_lambda\"] * sum(p.norm(2) for p in model.parameters())\n",
        "            loss += 0.001 * sum(p.abs().sum() for p in model.parameters())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss, val_tm = validate(model, val_loader, device)\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Time: {epoch_time:.1f}s | \"\n",
        "              f\"Train Loss: {train_loss/len(train_loader):.6f} | \"\n",
        "              f\"Val Loss: {val_loss:.6f} | TM-score: {val_tm:.4f}\")\n",
        "\n",
        "        if val_loss < best_val - config[\"min_delta\"]:\n",
        "            best_val = val_loss\n",
        "            best_tm = val_tm\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            print(f\"New best model saved (Val Loss: {val_loss:.6f}, TM-score: {val_tm:.4f})\")\n",
        "\n",
        "        if epoch - best_epoch > config[\"patience\"]:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Best validation loss: {best_val:.6f}\")\n",
        "    print(f\"Best TM-score: {best_tm:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "DQwdTU1MtjgN",
        "outputId": "e174cf5c-4ad0-4191-cf01-fc812a1fd40a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading full training dataset...\n",
            "Loaded 844 training samples\n",
            "\n",
            "Starting k-fold cross-validation...\n",
            "\n",
            "=== Fold 1/5 ===\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'float' and 'complex'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-30d77a4cffba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-30d77a4cffba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting k-fold cross-validation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0mfold_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-30d77a4cffba>\u001b[0m in \u001b[0;36mkfold_validation\u001b[0;34m(train_dataset, config, device)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-30d77a4cffba>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mbatch_tm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_tm_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mtm_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-30d77a4cffba>\u001b[0m in \u001b[0;36mcompute_tm_score\u001b[0;34m(pred_coords, true_coords, lengths)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mptr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0md0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mdist_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'float' and 'complex'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib"
      ],
      "metadata": {
        "id": "y-vo6JXeUQPj",
        "outputId": "6d4cad75-a42c-4855-d8fb-67e0c9e65cf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yOGektnDUjC9",
        "outputId": "53e594d9-fee2-4ca1-b2f3-08aa1a571cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Then save it to your drive\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/GVP_Model.pth')\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/GVP_Scaler.save')\n",
        "\n",
        "print(\"Saved to Google Drive as 'GVP_Model.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxJQGVM-TT1n",
        "outputId": "a9554c22-705f-4427-8245-47b2507f54b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to Google Drive as 'GVP_Model.pth'\n"
          ]
        }
      ]
    }
  ]
}