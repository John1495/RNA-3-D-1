{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPqBcFDp9wX3azA9EiDGESv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John1495/RNA-3-D-1/blob/main/GVP_model_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lqj9JG5NV0lQ",
        "outputId": "5dd0d80f-b182-49ca-ca82-6777fe3173cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_scatter-2.1.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.4/500.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_sparse-0.6.18%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_cluster-1.6.3%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (753 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt21cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt21cpu torch-sparse-0.6.18+pt21cpu torch-spline-conv-1.2.2+pt21cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "76gvpT5z7bjS",
        "outputId": "65a27f2c-e4c2-4de4-846b-30d316a4ee4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy matplotlib py3Dmol\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kJKvQQMgrUkI",
        "outputId": "2f4e1ef3-7a96-44e1-a6a4-904f44a556e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting py3Dmol\n",
            "  Downloading py3Dmol-2.4.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading py3Dmol-2.4.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: py3Dmol\n",
            "Successfully installed py3Dmol-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch\n",
        "!pip install torch==<your_version>+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uNTlN1FXnjrD",
        "outputId": "927a9be3-1b25-4830-a37e-dbe7e000e093"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cpu\n",
            "Uninstalling torch-2.6.0+cpu:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.11/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch-2.6.0+cpu.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: your_version: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B84Wmi02hSiv",
        "outputId": "77cf160a-6e28-4b5a-c968-054dc4f4bc6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Downloading greenlet-3.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading greenlet-3.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (583 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m583.9/583.9 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, colorlog, sqlalchemy, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 greenlet-3.2.1 optuna-4.3.0 sqlalchemy-2.0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch_geometric optuna matplotlib biopython\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuDIIMROi1Hs",
        "outputId": "9899b0a1-fd49-40ab-c983-bf9cfdf48adb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "sequences_df = pd.read_csv('/kaggle/cleaned_train_sequences2 (1).csv')\n",
        "labels_df = pd.read_csv('/kaggle/train_labels1.csv')\n",
        "\n",
        "print(\"Sequences:\")\n",
        "print(sequences_df.head(3))\n",
        "\n",
        "print(\"\\nLabels:\")\n",
        "print(labels_df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHaz1_1hYjcL",
        "outputId": "7a7c0080-a479-4900-be0f-2f981925d5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences:\n",
            "  target_id                            sequence temporal_cutoff  \\\n",
            "0    1SCL_A       GGGUGCUCAGUACGAGAGGAACCGCACCC      1995-01-26   \n",
            "1    1RNK_A  GGCGCAGUGGGCUAGCGCCACUCAAAAGGCCCAU      1995-02-27   \n",
            "2    1RHT_A            GGGACUGACGAUCACGCAGUCUAU      1995-06-03   \n",
            "\n",
            "                                         description  \\\n",
            "0               THE SARCIN-RICIN LOOP, A MODULAR RNA   \n",
            "1  THE STRUCTURE OF AN RNA PSEUDOKNOT THAT CAUSES...   \n",
            "2  24-MER RNA HAIRPIN COAT PROTEIN BINDING SITE F...   \n",
            "\n",
            "                                       all_sequences  \n",
            "0  >1SCL_1|Chain A|RNA SARCIN-RICIN LOOP|Rattus n...  \n",
            "1  >1RNK_1|Chain A|RNA PSEUDOKNOT|null\\nGGCGCAGUG...  \n",
            "2  >1RHT_1|Chain A|RNA (5'-R(P*GP*GP*GP*AP*CP*UP*...  \n",
            "\n",
            "Labels:\n",
            "   resid         ID resname        x_1        y_1    z_1\n",
            "0      1   17RA_A_1       G  35.856998 -10.769000 -7.548\n",
            "1     10  17RA_A_10       A  26.341999  12.365000 -0.594\n",
            "2     11  17RA_A_11       U  23.917999  16.023001 -5.418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dataset class to load RNA sequence and labels\n",
        "class RNAGraphDataset(Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "\n",
        "        self.graphs = self.build_graphs()\n",
        "\n",
        "    def build_graphs(self):\n",
        "        graphs = []\n",
        "        label_dict = self.labels.groupby(\"ID\")\n",
        "\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            node_feats = self.encode_sequence(sequence)\n",
        "\n",
        "            # Match label\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue  # Skip mismatched\n",
        "\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "\n",
        "            # Edge index (chain sequentially)\n",
        "            edge_index = self.build_edge_index(len(sequence))\n",
        "\n",
        "            data = Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float),\n",
        "                edge_index=edge_index,\n",
        "                y=torch.tensor(coords, dtype=torch.float)\n",
        "            )\n",
        "            graphs.append(data)\n",
        "\n",
        "        return graphs\n",
        "\n",
        "    def encode_sequence(self, sequence):\n",
        "        mapping = {'A': [1,0,0,0], 'U': [0,1,0,0], 'G': [0,0,1,0], 'C': [0,0,0,1]}\n",
        "        return [mapping.get(nt, [0,0,0,0]) for nt in sequence]\n",
        "\n",
        "    def build_edge_index(self, length):\n",
        "        edge_index = []\n",
        "        for i in range(length - 1):\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])  # Add bidirectional edges\n",
        "        return torch.tensor(edge_index).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "\n",
        "# GVP-like model using GCNConv as a placeholder\n",
        "class GVPModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels=4, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = torch.nn.Linear(hidden_channels, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Custom collate function to handle batching of Data objects\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            loss = F.mse_loss(pred, data.y)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Main function to run training and validation\n",
        "def main():\n",
        "    # File paths for train and validation data\n",
        "    train_sequences = '/kaggle/cleaned_train_sequences2 (1).csv'\n",
        "    train_labels = '/kaggle/train_labels1.csv'\n",
        "    validation_sequences = '/kaggle/validation_sequences.csv'\n",
        "    validation_labels = '/kaggle/validation_labels.csv'\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = RNAGraphDataset(train_sequences, train_labels)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"No valid training data found. Check sequence-label matching.\")\n",
        "\n",
        "    # Random split the dataset into train and validation sets\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create DataLoader for train and validation sets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=collate_fn)\n",
        "\n",
        "    # Setup device and model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GVPModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Track the best model based on validation loss\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(1, 101):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        val_loss = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Save the model with the best validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_gvp_model.pt\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM1e7jD-dYKw",
        "outputId": "242b0945-6a3a-4af7-d6ee-aa5b929e29dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 9696.4653, Val Loss = 9952.4718\n",
            "Epoch 2: Train Loss = 8930.3638, Val Loss = 9792.6099\n",
            "Epoch 3: Train Loss = 8913.2559, Val Loss = 9873.7861\n",
            "Epoch 4: Train Loss = 8909.9442, Val Loss = 9770.0099\n",
            "Epoch 5: Train Loss = 8911.4973, Val Loss = 9890.3780\n",
            "Epoch 6: Train Loss = 8864.6202, Val Loss = 9721.2444\n",
            "Epoch 7: Train Loss = 8925.6557, Val Loss = 9812.3279\n",
            "Epoch 8: Train Loss = 8903.7461, Val Loss = 9838.0926\n",
            "Epoch 9: Train Loss = 8900.9657, Val Loss = 9889.6365\n",
            "Epoch 10: Train Loss = 8896.2538, Val Loss = 9728.6678\n",
            "Epoch 11: Train Loss = 8899.6930, Val Loss = 9822.0760\n",
            "Epoch 12: Train Loss = 8904.7194, Val Loss = 9904.1818\n",
            "Epoch 13: Train Loss = 8901.4221, Val Loss = 9733.9354\n",
            "Epoch 14: Train Loss = 8892.1657, Val Loss = 9923.2250\n",
            "Epoch 15: Train Loss = 8902.3542, Val Loss = 9939.7261\n",
            "Epoch 16: Train Loss = 8912.3565, Val Loss = 9854.1458\n",
            "Epoch 17: Train Loss = 8908.6952, Val Loss = 9818.4915\n",
            "Epoch 18: Train Loss = 8903.2166, Val Loss = 9831.9160\n",
            "Epoch 19: Train Loss = 8899.1962, Val Loss = 9907.9214\n",
            "Epoch 20: Train Loss = 8920.7276, Val Loss = 9816.7008\n",
            "Epoch 21: Train Loss = 8902.9064, Val Loss = 9889.1578\n",
            "Epoch 22: Train Loss = 8913.1795, Val Loss = 9802.3816\n",
            "Epoch 23: Train Loss = 8892.2728, Val Loss = 9934.5288\n",
            "Epoch 24: Train Loss = 8910.1967, Val Loss = 9807.1719\n",
            "Epoch 25: Train Loss = 8909.6574, Val Loss = 9864.4154\n",
            "Epoch 26: Train Loss = 8921.6728, Val Loss = 9816.3484\n",
            "Epoch 27: Train Loss = 8896.9608, Val Loss = 9751.5622\n",
            "Epoch 28: Train Loss = 8894.8259, Val Loss = 9919.3956\n",
            "Epoch 29: Train Loss = 8900.5483, Val Loss = 9989.2195\n",
            "Epoch 30: Train Loss = 8902.9528, Val Loss = 9965.6279\n",
            "Epoch 31: Train Loss = 8906.5126, Val Loss = 9901.2677\n",
            "Epoch 32: Train Loss = 8910.2010, Val Loss = 9749.4853\n",
            "Epoch 33: Train Loss = 8879.2012, Val Loss = 9850.9139\n",
            "Epoch 34: Train Loss = 8887.4619, Val Loss = 9728.0516\n",
            "Epoch 35: Train Loss = 8910.5115, Val Loss = 9758.7091\n",
            "Epoch 36: Train Loss = 8903.9259, Val Loss = 9852.5425\n",
            "Epoch 37: Train Loss = 8883.8338, Val Loss = 9993.7836\n",
            "Epoch 38: Train Loss = 8910.3999, Val Loss = 9741.8518\n",
            "Epoch 39: Train Loss = 8917.7106, Val Loss = 9850.0111\n",
            "Epoch 40: Train Loss = 8872.3470, Val Loss = 10098.4476\n",
            "Epoch 41: Train Loss = 8913.5868, Val Loss = 9791.2415\n",
            "Epoch 42: Train Loss = 8906.1851, Val Loss = 9855.1866\n",
            "Epoch 43: Train Loss = 8909.9935, Val Loss = 9848.5561\n",
            "Epoch 44: Train Loss = 8899.9290, Val Loss = 9818.4601\n",
            "Epoch 45: Train Loss = 8890.6708, Val Loss = 9973.4090\n",
            "Epoch 46: Train Loss = 8887.7501, Val Loss = 9888.3041\n",
            "Epoch 47: Train Loss = 8904.4150, Val Loss = 9888.8075\n",
            "Epoch 48: Train Loss = 8906.5844, Val Loss = 9904.1270\n",
            "Epoch 49: Train Loss = 8907.0467, Val Loss = 9869.0014\n",
            "Epoch 50: Train Loss = 8907.6487, Val Loss = 9786.2784\n",
            "Epoch 51: Train Loss = 8900.9081, Val Loss = 9862.9092\n",
            "Epoch 52: Train Loss = 8900.8594, Val Loss = 9804.9157\n",
            "Epoch 53: Train Loss = 8907.7242, Val Loss = 9880.8393\n",
            "Epoch 54: Train Loss = 8915.7346, Val Loss = 9811.8119\n",
            "Epoch 55: Train Loss = 8903.7556, Val Loss = 9770.0594\n",
            "Epoch 56: Train Loss = 8901.6671, Val Loss = 9873.5710\n",
            "Epoch 57: Train Loss = 8892.8410, Val Loss = 9789.1105\n",
            "Epoch 58: Train Loss = 8887.0014, Val Loss = 9967.1818\n",
            "Epoch 59: Train Loss = 8897.8171, Val Loss = 9896.5511\n",
            "Epoch 60: Train Loss = 8886.0029, Val Loss = 10039.3413\n",
            "Epoch 61: Train Loss = 8898.7841, Val Loss = 9744.1609\n",
            "Epoch 62: Train Loss = 8896.6566, Val Loss = 9745.1166\n",
            "Epoch 63: Train Loss = 8911.2835, Val Loss = 9911.6624\n",
            "Epoch 64: Train Loss = 8909.3709, Val Loss = 9830.9758\n",
            "Epoch 65: Train Loss = 8891.6741, Val Loss = 9846.4174\n",
            "Epoch 66: Train Loss = 8897.1888, Val Loss = 9814.7208\n",
            "Epoch 67: Train Loss = 8904.8200, Val Loss = 9902.1411\n",
            "Epoch 68: Train Loss = 8895.2667, Val Loss = 9914.7961\n",
            "Epoch 69: Train Loss = 8911.9849, Val Loss = 9904.7827\n",
            "Epoch 70: Train Loss = 8889.0489, Val Loss = 10092.0877\n",
            "Epoch 71: Train Loss = 8907.4326, Val Loss = 9827.3336\n",
            "Epoch 72: Train Loss = 8880.3811, Val Loss = 9707.7231\n",
            "Epoch 73: Train Loss = 8914.1998, Val Loss = 9828.2390\n",
            "Epoch 74: Train Loss = 8906.4952, Val Loss = 9825.1677\n",
            "Epoch 75: Train Loss = 8881.5971, Val Loss = 9990.0418\n",
            "Epoch 76: Train Loss = 8844.4406, Val Loss = 9706.6846\n",
            "Epoch 77: Train Loss = 8913.4195, Val Loss = 9840.6453\n",
            "Epoch 78: Train Loss = 8903.7198, Val Loss = 9778.1554\n",
            "Epoch 79: Train Loss = 8908.1244, Val Loss = 9814.9299\n",
            "Epoch 80: Train Loss = 8905.4254, Val Loss = 9834.1430\n",
            "Epoch 81: Train Loss = 8895.4158, Val Loss = 9865.0368\n",
            "Epoch 82: Train Loss = 8896.1833, Val Loss = 9963.6424\n",
            "Epoch 83: Train Loss = 8887.5052, Val Loss = 10026.6492\n",
            "Epoch 84: Train Loss = 8905.5039, Val Loss = 9890.3285\n",
            "Epoch 85: Train Loss = 8908.7515, Val Loss = 9796.0859\n",
            "Epoch 86: Train Loss = 8903.7080, Val Loss = 9988.8192\n",
            "Epoch 87: Train Loss = 8905.6043, Val Loss = 9769.9206\n",
            "Epoch 88: Train Loss = 8884.6109, Val Loss = 9796.3449\n",
            "Epoch 89: Train Loss = 8901.6346, Val Loss = 9840.8978\n",
            "Epoch 90: Train Loss = 8894.2303, Val Loss = 9995.4204\n",
            "Epoch 91: Train Loss = 8891.3290, Val Loss = 9741.0733\n",
            "Epoch 92: Train Loss = 8910.4949, Val Loss = 9862.8544\n",
            "Epoch 93: Train Loss = 8894.3595, Val Loss = 9749.2784\n",
            "Epoch 94: Train Loss = 8889.7655, Val Loss = 9705.5142\n",
            "Epoch 95: Train Loss = 8880.8763, Val Loss = 9795.2577\n",
            "Epoch 96: Train Loss = 8891.6426, Val Loss = 9830.0807\n",
            "Epoch 97: Train Loss = 8895.9830, Val Loss = 9720.1571\n",
            "Epoch 98: Train Loss = 8898.9856, Val Loss = 9805.9986\n",
            "Epoch 99: Train Loss = 8912.6772, Val Loss = 9868.4338\n",
            "Epoch 100: Train Loss = 8896.8262, Val Loss = 9917.4280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dataset class to load RNA sequence and labels\n",
        "class RNAGraphDataset(Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "        self.graphs = self.build_graphs()\n",
        "\n",
        "    def build_graphs(self):\n",
        "        graphs = []\n",
        "        label_dict = self.labels.groupby(\"ID\")\n",
        "\n",
        "        all_coords = []\n",
        "        # First pass to collect stats\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            all_coords.append(coords)\n",
        "\n",
        "        all_coords = np.concatenate(all_coords, axis=0)\n",
        "        self.mean = all_coords.mean(axis=0)\n",
        "        self.std = all_coords.std(axis=0)\n",
        "\n",
        "        # Second pass to build graphs\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            node_feats = self.encode_sequence(sequence)\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "            edge_index = self.build_edge_index(len(sequence))\n",
        "            data = Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float),\n",
        "                edge_index=edge_index,\n",
        "                y=torch.tensor(coords, dtype=torch.float)\n",
        "            )\n",
        "            graphs.append(data)\n",
        "        return graphs\n",
        "\n",
        "    def encode_sequence(self, sequence):\n",
        "        mapping = {'A': [1,0,0,0], 'U': [0,1,0,0], 'G': [0,0,1,0], 'C': [0,0,0,1]}\n",
        "        return [mapping.get(nt, [0,0,0,0]) for nt in sequence]\n",
        "\n",
        "    def build_edge_index(self, length):\n",
        "        edge_index = []\n",
        "        for i in range(length - 1):\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "        return torch.tensor(edge_index).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "\n",
        "# Enhanced GVP-like model using GCNConv\n",
        "class GVPModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels=4, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.lin1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = torch.nn.Linear(hidden_channels, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.dropout(F.relu(self.conv1(x, edge_index)))\n",
        "        x = self.dropout(F.relu(self.conv2(x, edge_index)))\n",
        "        x = self.dropout(F.relu(self.conv3(x, edge_index)))\n",
        "        x = self.dropout(F.relu(self.lin1(x)))\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Custom collate function to handle batching\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "\n",
        "# Training function with gradient clipping\n",
        "\n",
        "def train(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred, data.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            loss = F.mse_loss(pred, data.y)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Main training loop\n",
        "\n",
        "def main():\n",
        "    train_sequences = '/kaggle/cleaned_train_sequences2 (1).csv'\n",
        "    train_labels = '/kaggle/train_labels1.csv'\n",
        "    validation_sequences = '/kaggle/validation_sequences.csv'\n",
        "    validation_labels = '/kaggle/validation_labels.csv'\n",
        "\n",
        "    dataset = RNAGraphDataset(train_sequences, train_labels)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"No valid training data found. Check sequence-label matching.\")\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GVPModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    for epoch in range(1, 101):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        val_loss = evaluate(model, val_loader, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_gvp_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter > 10:\n",
        "                print(\"Early stopping due to no improvement in validation loss.\")\n",
        "                break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkpSyUk8jlSa",
        "outputId": "44284887-6a86-47df-c47e-9a87262c129a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.7275, Val Loss = 0.7240\n",
            "Epoch 2: Train Loss = 0.6815, Val Loss = 0.7248\n",
            "Epoch 3: Train Loss = 0.7296, Val Loss = 0.7246\n",
            "Epoch 4: Train Loss = 0.7082, Val Loss = 0.7335\n",
            "Epoch 5: Train Loss = 0.7271, Val Loss = 0.7211\n",
            "Epoch 6: Train Loss = 0.7620, Val Loss = 0.7262\n",
            "Epoch 7: Train Loss = 0.7057, Val Loss = 0.7250\n",
            "Epoch 8: Train Loss = 0.7533, Val Loss = 0.7227\n",
            "Epoch 9: Train Loss = 0.7422, Val Loss = 0.7225\n",
            "Epoch 10: Train Loss = 0.6937, Val Loss = 0.7221\n",
            "Epoch 11: Train Loss = 0.7104, Val Loss = 0.7201\n",
            "Epoch 12: Train Loss = 0.7302, Val Loss = 0.7201\n",
            "Epoch 13: Train Loss = 0.7182, Val Loss = 0.7238\n",
            "Epoch 14: Train Loss = 0.7485, Val Loss = 0.7239\n",
            "Epoch 15: Train Loss = 0.7336, Val Loss = 0.7217\n",
            "Epoch 16: Train Loss = 0.7581, Val Loss = 0.7210\n",
            "Epoch 17: Train Loss = 0.7525, Val Loss = 0.7294\n",
            "Epoch 18: Train Loss = 0.6777, Val Loss = 0.7232\n",
            "Epoch 19: Train Loss = 0.7319, Val Loss = 0.7220\n",
            "Epoch 20: Train Loss = 0.6822, Val Loss = 0.7191\n",
            "Epoch 21: Train Loss = 0.7505, Val Loss = 0.7190\n",
            "Epoch 22: Train Loss = 0.7044, Val Loss = 0.7203\n",
            "Epoch 23: Train Loss = 0.6659, Val Loss = 0.7208\n",
            "Epoch 24: Train Loss = 0.7464, Val Loss = 0.7209\n",
            "Epoch 25: Train Loss = 0.6897, Val Loss = 0.7216\n",
            "Epoch 26: Train Loss = 0.6984, Val Loss = 0.7200\n",
            "Epoch 27: Train Loss = 0.7145, Val Loss = 0.7221\n",
            "Epoch 28: Train Loss = 0.7274, Val Loss = 0.7215\n",
            "Epoch 29: Train Loss = 0.7365, Val Loss = 0.7203\n",
            "Epoch 30: Train Loss = 0.7293, Val Loss = 0.7207\n",
            "Epoch 31: Train Loss = 0.7039, Val Loss = 0.7202\n",
            "Epoch 32: Train Loss = 0.7084, Val Loss = 0.7197\n",
            "Early stopping due to no improvement in validation loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import warnings\n",
        "\n",
        "# Suppress unnecessary warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset Preparation\n",
        "# ----------------------------\n",
        "class RNAGraphDataset(Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.graphs = self._process_data()\n",
        "\n",
        "    def _process_data(self):\n",
        "        graphs = []\n",
        "        all_coords = []\n",
        "\n",
        "        # First pass: collect statistics\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if not matched_labels.empty and len(matched_labels) == len(row['sequence']):\n",
        "                coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "                all_coords.append(coords)\n",
        "\n",
        "        all_coords = np.concatenate(all_coords, axis=0)\n",
        "        self.mean = all_coords.mean(axis=0)\n",
        "        self.std = all_coords.std(axis=0) + 1e-8  # Prevent division by zero\n",
        "\n",
        "        # Second pass: build graphs\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "\n",
        "            node_feats = self._encode_sequence(sequence)\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "\n",
        "            edge_index = self._build_edges(len(sequence))\n",
        "\n",
        "            graphs.append(Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float32),\n",
        "                edge_index=edge_index,\n",
        "                y=torch.tensor(coords, dtype=torch.float32)\n",
        "            ))\n",
        "        return graphs\n",
        "\n",
        "    def _encode_sequence(self, sequence):\n",
        "        mapping = {'A': [1,0,0,0], 'U': [0,1,0,0],\n",
        "                  'G': [0,0,1,0], 'C': [0,0,0,1]}\n",
        "        return [mapping.get(nt, [0,0,0,0]) for nt in sequence]\n",
        "\n",
        "    def _build_edges(self, length):\n",
        "        edge_index = []\n",
        "        for i in range(length - 1):\n",
        "            # Add bidirectional edges\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "        return torch.tensor(edge_index).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced GNN Model (Fixed)\n",
        "# ----------------------------\n",
        "class RNA3DModel(nn.Module):\n",
        "    def __init__(self, in_channels=4, hidden_channels=256, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        # Output layer\n",
        "        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels//2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_channels//2, 3)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Message passing without edge attributes\n",
        "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
        "            x = conv(x, edge_index)\n",
        "            if i != len(self.convs) - 1:\n",
        "                x = bn(x)\n",
        "                x = F.leaky_relu(x, 0.2)\n",
        "                x = self.dropout(x)\n",
        "\n",
        "        return self.fc(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Utilities\n",
        "# ----------------------------\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, grad_accum=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, data in enumerate(loader):\n",
        "        data = data.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred, data.y) / grad_accum\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % grad_accum == 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * grad_accum\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            total_loss += F.mse_loss(pred, data.y).item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training Loop\n",
        "# ----------------------------\n",
        "def main():\n",
        "    # File paths (updated with your Kaggle paths)\n",
        "    train_sequences = '/kaggle/cleaned_train_sequences2 (1).csv'\n",
        "    train_labels = '/kaggle/train_labels1.csv'\n",
        "    validation_sequences = '/kaggle/validation_sequences.csv'\n",
        "    validation_labels = '/kaggle/validation_labels.csv'\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'batch_size': 16,\n",
        "        'hidden_dim': 256,\n",
        "        'num_layers': 4,\n",
        "        'max_epochs': 100,\n",
        "        'patience': 20,\n",
        "        'min_delta': 0.001,\n",
        "        'grad_accum': 4\n",
        "    }\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data loading - using separate train/validation files\n",
        "    train_dataset = RNAGraphDataset(train_sequences, train_labels)\n",
        "    val_dataset = RNAGraphDataset(validation_sequences, validation_labels)\n",
        "\n",
        "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "        raise ValueError(\"No valid training/validation data found!\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Model setup\n",
        "    model = RNA3DModel(\n",
        "        hidden_channels=config['hidden_dim'],\n",
        "        num_layers=config['num_layers']\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=5e-3,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=config['max_epochs'],\n",
        "        pct_start=0.2\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val = float('inf')\n",
        "    patience = config['patience']\n",
        "\n",
        "    for epoch in range(1, config['max_epochs'] + 1):\n",
        "        train_loss = train_epoch(\n",
        "            model, train_loader, optimizer, device,\n",
        "            grad_accum=config['grad_accum']\n",
        "        )\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | \"\n",
        "              f\"Train Loss: {train_loss:.6f} | \"\n",
        "              f\"Val Loss: {val_loss:.6f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < (best_val - config['min_delta']):\n",
        "            best_val = val_loss\n",
        "            patience = config['patience']\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            print(f\"New best model saved (Val Loss: {val_loss:.6f})\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N907Ngs11EsP",
        "outputId": "63d25caa-8ef8-4339-ea7c-b9113c3e4635"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model parameters: 233,475\n",
            "Epoch 001 | Train Loss: 163.425323 | Val Loss: 10.179984 | LR: 2.00e-04\n",
            "New best model saved (Val Loss: 10.179984)\n",
            "Epoch 002 | Train Loss: 112.385022 | Val Loss: 18.906967 | LR: 2.00e-04\n",
            "Epoch 003 | Train Loss: 88.193440 | Val Loss: 10.859036 | LR: 2.00e-04\n",
            "Epoch 004 | Train Loss: 69.011355 | Val Loss: 9.162365 | LR: 2.00e-04\n",
            "New best model saved (Val Loss: 9.162365)\n",
            "Epoch 005 | Train Loss: 54.302818 | Val Loss: 7.411708 | LR: 2.00e-04\n",
            "New best model saved (Val Loss: 7.411708)\n",
            "Epoch 006 | Train Loss: 42.759427 | Val Loss: 5.990111 | LR: 2.00e-04\n",
            "New best model saved (Val Loss: 5.990111)\n",
            "Epoch 007 | Train Loss: 33.843796 | Val Loss: 4.987537 | LR: 2.01e-04\n",
            "New best model saved (Val Loss: 4.987537)\n",
            "Epoch 008 | Train Loss: 26.763395 | Val Loss: 4.120634 | LR: 2.01e-04\n",
            "New best model saved (Val Loss: 4.120634)\n",
            "Epoch 009 | Train Loss: 21.307909 | Val Loss: 4.186143 | LR: 2.01e-04\n",
            "Epoch 010 | Train Loss: 17.164058 | Val Loss: 3.697249 | LR: 2.01e-04\n",
            "New best model saved (Val Loss: 3.697249)\n",
            "Epoch 011 | Train Loss: 14.273009 | Val Loss: 3.373197 | LR: 2.01e-04\n",
            "New best model saved (Val Loss: 3.373197)\n",
            "Epoch 012 | Train Loss: 12.086180 | Val Loss: 3.408998 | LR: 2.02e-04\n",
            "Epoch 013 | Train Loss: 10.559760 | Val Loss: 2.884252 | LR: 2.02e-04\n",
            "New best model saved (Val Loss: 2.884252)\n",
            "Epoch 014 | Train Loss: 9.315645 | Val Loss: 2.978805 | LR: 2.02e-04\n",
            "Epoch 015 | Train Loss: 8.541475 | Val Loss: 2.881689 | LR: 2.02e-04\n",
            "New best model saved (Val Loss: 2.881689)\n",
            "Epoch 016 | Train Loss: 7.780400 | Val Loss: 2.519254 | LR: 2.03e-04\n",
            "New best model saved (Val Loss: 2.519254)\n",
            "Epoch 017 | Train Loss: 7.405621 | Val Loss: 2.659787 | LR: 2.03e-04\n",
            "Epoch 018 | Train Loss: 6.878254 | Val Loss: 2.305016 | LR: 2.03e-04\n",
            "New best model saved (Val Loss: 2.305016)\n",
            "Epoch 019 | Train Loss: 6.521625 | Val Loss: 2.255090 | LR: 2.04e-04\n",
            "New best model saved (Val Loss: 2.255090)\n",
            "Epoch 020 | Train Loss: 6.246051 | Val Loss: 1.914052 | LR: 2.04e-04\n",
            "New best model saved (Val Loss: 1.914052)\n",
            "Epoch 021 | Train Loss: 5.854335 | Val Loss: 2.134588 | LR: 2.05e-04\n",
            "Epoch 022 | Train Loss: 5.623451 | Val Loss: 1.937778 | LR: 2.05e-04\n",
            "Epoch 023 | Train Loss: 5.315060 | Val Loss: 1.812403 | LR: 2.06e-04\n",
            "New best model saved (Val Loss: 1.812403)\n",
            "Epoch 024 | Train Loss: 5.116792 | Val Loss: 1.828893 | LR: 2.06e-04\n",
            "Epoch 025 | Train Loss: 5.035668 | Val Loss: 1.834931 | LR: 2.07e-04\n",
            "Epoch 026 | Train Loss: 4.826714 | Val Loss: 1.651554 | LR: 2.07e-04\n",
            "New best model saved (Val Loss: 1.651554)\n",
            "Epoch 027 | Train Loss: 4.628453 | Val Loss: 1.705810 | LR: 2.08e-04\n",
            "Epoch 028 | Train Loss: 4.333299 | Val Loss: 1.544258 | LR: 2.08e-04\n",
            "New best model saved (Val Loss: 1.544258)\n",
            "Epoch 029 | Train Loss: 4.238678 | Val Loss: 1.555012 | LR: 2.09e-04\n",
            "Epoch 030 | Train Loss: 4.120123 | Val Loss: 1.590363 | LR: 2.09e-04\n",
            "Epoch 031 | Train Loss: 4.040087 | Val Loss: 1.410339 | LR: 2.10e-04\n",
            "New best model saved (Val Loss: 1.410339)\n",
            "Epoch 032 | Train Loss: 3.880418 | Val Loss: 1.613993 | LR: 2.11e-04\n",
            "Epoch 033 | Train Loss: 3.778392 | Val Loss: 1.341711 | LR: 2.11e-04\n",
            "New best model saved (Val Loss: 1.341711)\n",
            "Epoch 034 | Train Loss: 3.693929 | Val Loss: 1.489285 | LR: 2.12e-04\n",
            "Epoch 035 | Train Loss: 3.601667 | Val Loss: 1.442907 | LR: 2.13e-04\n",
            "Epoch 036 | Train Loss: 3.406721 | Val Loss: 1.298503 | LR: 2.14e-04\n",
            "New best model saved (Val Loss: 1.298503)\n",
            "Epoch 037 | Train Loss: 3.245836 | Val Loss: 1.332692 | LR: 2.14e-04\n",
            "Epoch 038 | Train Loss: 3.131558 | Val Loss: 1.345918 | LR: 2.15e-04\n",
            "Epoch 039 | Train Loss: 3.133611 | Val Loss: 1.253161 | LR: 2.16e-04\n",
            "New best model saved (Val Loss: 1.253161)\n",
            "Epoch 040 | Train Loss: 2.945532 | Val Loss: 1.310024 | LR: 2.17e-04\n",
            "Epoch 041 | Train Loss: 2.863798 | Val Loss: 1.256601 | LR: 2.18e-04\n",
            "Epoch 042 | Train Loss: 2.868011 | Val Loss: 1.210066 | LR: 2.19e-04\n",
            "New best model saved (Val Loss: 1.210066)\n",
            "Epoch 043 | Train Loss: 2.866338 | Val Loss: 1.236510 | LR: 2.20e-04\n",
            "Epoch 044 | Train Loss: 2.689921 | Val Loss: 1.281529 | LR: 2.20e-04\n",
            "Epoch 045 | Train Loss: 2.593510 | Val Loss: 1.187034 | LR: 2.21e-04\n",
            "New best model saved (Val Loss: 1.187034)\n",
            "Epoch 046 | Train Loss: 2.599550 | Val Loss: 1.224536 | LR: 2.22e-04\n",
            "Epoch 047 | Train Loss: 2.583231 | Val Loss: 1.215101 | LR: 2.23e-04\n",
            "Epoch 048 | Train Loss: 2.487730 | Val Loss: 1.175814 | LR: 2.24e-04\n",
            "New best model saved (Val Loss: 1.175814)\n",
            "Epoch 049 | Train Loss: 2.509909 | Val Loss: 1.203286 | LR: 2.25e-04\n",
            "Epoch 050 | Train Loss: 2.390650 | Val Loss: 1.124029 | LR: 2.26e-04\n",
            "New best model saved (Val Loss: 1.124029)\n",
            "Epoch 051 | Train Loss: 2.247614 | Val Loss: 1.211870 | LR: 2.27e-04\n",
            "Epoch 052 | Train Loss: 2.333621 | Val Loss: 1.146953 | LR: 2.28e-04\n",
            "Epoch 053 | Train Loss: 2.316165 | Val Loss: 1.122032 | LR: 2.30e-04\n",
            "New best model saved (Val Loss: 1.122032)\n",
            "Epoch 054 | Train Loss: 2.215062 | Val Loss: 1.126562 | LR: 2.31e-04\n",
            "Epoch 055 | Train Loss: 2.231844 | Val Loss: 1.118911 | LR: 2.32e-04\n",
            "New best model saved (Val Loss: 1.118911)\n",
            "Epoch 056 | Train Loss: 2.162601 | Val Loss: 1.130642 | LR: 2.33e-04\n",
            "Epoch 057 | Train Loss: 2.192863 | Val Loss: 1.091141 | LR: 2.34e-04\n",
            "New best model saved (Val Loss: 1.091141)\n",
            "Epoch 058 | Train Loss: 1.934950 | Val Loss: 1.111697 | LR: 2.35e-04\n",
            "Epoch 059 | Train Loss: 2.085157 | Val Loss: 1.125038 | LR: 2.37e-04\n",
            "Epoch 060 | Train Loss: 2.005473 | Val Loss: 1.095332 | LR: 2.38e-04\n",
            "Epoch 061 | Train Loss: 1.943178 | Val Loss: 1.085232 | LR: 2.39e-04\n",
            "New best model saved (Val Loss: 1.085232)\n",
            "Epoch 062 | Train Loss: 1.959302 | Val Loss: 1.057963 | LR: 2.40e-04\n",
            "New best model saved (Val Loss: 1.057963)\n",
            "Epoch 063 | Train Loss: 1.848351 | Val Loss: 1.111551 | LR: 2.42e-04\n",
            "Epoch 064 | Train Loss: 1.863084 | Val Loss: 1.087136 | LR: 2.43e-04\n",
            "Epoch 065 | Train Loss: 1.915317 | Val Loss: 1.062175 | LR: 2.44e-04\n",
            "Epoch 066 | Train Loss: 1.853716 | Val Loss: 1.102401 | LR: 2.46e-04\n",
            "Epoch 067 | Train Loss: 1.714594 | Val Loss: 1.084758 | LR: 2.47e-04\n",
            "Epoch 068 | Train Loss: 1.708073 | Val Loss: 1.066756 | LR: 2.49e-04\n",
            "Epoch 069 | Train Loss: 1.787144 | Val Loss: 1.048558 | LR: 2.50e-04\n",
            "New best model saved (Val Loss: 1.048558)\n",
            "Epoch 070 | Train Loss: 1.679798 | Val Loss: 1.062872 | LR: 2.52e-04\n",
            "Epoch 071 | Train Loss: 1.646568 | Val Loss: 1.064137 | LR: 2.53e-04\n",
            "Epoch 072 | Train Loss: 1.739783 | Val Loss: 1.051588 | LR: 2.55e-04\n",
            "Epoch 073 | Train Loss: 1.537642 | Val Loss: 1.065044 | LR: 2.56e-04\n",
            "Epoch 074 | Train Loss: 1.680151 | Val Loss: 1.058110 | LR: 2.58e-04\n",
            "Epoch 075 | Train Loss: 1.553885 | Val Loss: 1.052401 | LR: 2.59e-04\n",
            "Epoch 076 | Train Loss: 1.478456 | Val Loss: 1.040655 | LR: 2.61e-04\n",
            "New best model saved (Val Loss: 1.040655)\n",
            "Epoch 077 | Train Loss: 1.620115 | Val Loss: 1.040330 | LR: 2.62e-04\n",
            "Epoch 078 | Train Loss: 1.594312 | Val Loss: 1.031849 | LR: 2.64e-04\n",
            "New best model saved (Val Loss: 1.031849)\n",
            "Epoch 079 | Train Loss: 1.463152 | Val Loss: 1.048466 | LR: 2.66e-04\n",
            "Epoch 080 | Train Loss: 1.476023 | Val Loss: 1.045468 | LR: 2.67e-04\n",
            "Epoch 081 | Train Loss: 1.492903 | Val Loss: 1.043506 | LR: 2.69e-04\n",
            "Epoch 082 | Train Loss: 1.561911 | Val Loss: 1.050616 | LR: 2.71e-04\n",
            "Epoch 083 | Train Loss: 1.371204 | Val Loss: 1.036432 | LR: 2.72e-04\n",
            "Epoch 084 | Train Loss: 1.448766 | Val Loss: 1.028707 | LR: 2.74e-04\n",
            "New best model saved (Val Loss: 1.028707)\n",
            "Epoch 085 | Train Loss: 1.377949 | Val Loss: 1.035048 | LR: 2.76e-04\n",
            "Epoch 086 | Train Loss: 1.318019 | Val Loss: 1.032988 | LR: 2.78e-04\n",
            "Epoch 087 | Train Loss: 1.375168 | Val Loss: 1.038440 | LR: 2.79e-04\n",
            "Epoch 088 | Train Loss: 1.432812 | Val Loss: 1.025244 | LR: 2.81e-04\n",
            "New best model saved (Val Loss: 1.025244)\n",
            "Epoch 089 | Train Loss: 1.325226 | Val Loss: 1.025374 | LR: 2.83e-04\n",
            "Epoch 090 | Train Loss: 1.391179 | Val Loss: 1.021428 | LR: 2.85e-04\n",
            "New best model saved (Val Loss: 1.021428)\n",
            "Epoch 091 | Train Loss: 1.297061 | Val Loss: 1.029824 | LR: 2.87e-04\n",
            "Epoch 092 | Train Loss: 1.292417 | Val Loss: 1.036562 | LR: 2.89e-04\n",
            "Epoch 093 | Train Loss: 1.433430 | Val Loss: 1.025978 | LR: 2.91e-04\n",
            "Epoch 094 | Train Loss: 1.340251 | Val Loss: 1.022099 | LR: 2.93e-04\n",
            "Epoch 095 | Train Loss: 1.275396 | Val Loss: 1.016121 | LR: 2.95e-04\n",
            "New best model saved (Val Loss: 1.016121)\n",
            "Epoch 096 | Train Loss: 1.329955 | Val Loss: 1.021969 | LR: 2.97e-04\n",
            "Epoch 097 | Train Loss: 1.201595 | Val Loss: 1.018011 | LR: 2.99e-04\n",
            "Epoch 098 | Train Loss: 1.256295 | Val Loss: 1.019167 | LR: 3.01e-04\n",
            "Epoch 099 | Train Loss: 1.314819 | Val Loss: 1.019828 | LR: 3.03e-04\n",
            "Epoch 100 | Train Loss: 1.330360 | Val Loss: 1.018784 | LR: 3.05e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import warnings\n",
        "\n",
        "# Suppress unnecessary warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced Dataset with Edge Features\n",
        "# ----------------------------\n",
        "class RNAGraphDataset(Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.graphs = self._process_data()\n",
        "\n",
        "    def _process_data(self):\n",
        "        graphs = []\n",
        "        all_coords = []\n",
        "\n",
        "        # First pass: collect statistics\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if not matched_labels.empty and len(matched_labels) == len(row['sequence']):\n",
        "                coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "                all_coords.append(coords)\n",
        "\n",
        "        all_coords = np.concatenate(all_coords, axis=0)\n",
        "        self.mean = all_coords.mean(axis=0)\n",
        "        self.std = all_coords.std(axis=0) + 1e-8\n",
        "\n",
        "        # Second pass: build graphs with enhanced features\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "\n",
        "            node_feats = self._encode_sequence(sequence)\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "\n",
        "            edge_index, edge_attr = self._build_edges(len(sequence), coords)\n",
        "\n",
        "            graphs.append(Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float32),\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                y=torch.tensor(coords, dtype=torch.float32),\n",
        "                pos=torch.tensor(coords, dtype=torch.float32)  # Added positional encoding\n",
        "            ))\n",
        "        return graphs\n",
        "\n",
        "    def _encode_sequence(self, sequence):\n",
        "        # Enhanced nucleotide encoding\n",
        "        mapping = {\n",
        "            'A': [1,0,0,0, 0.1],  # Added chemical property dimension\n",
        "            'U': [0,1,0,0, 0.2],\n",
        "            'G': [0,0,1,0, 0.3],\n",
        "            'C': [0,0,0,1, 0.4]\n",
        "        }\n",
        "        return [mapping.get(nt, [0,0,0,0, 0.0]) for nt in sequence]\n",
        "\n",
        "    def _build_edges(self, length, coords):\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        for i in range(length - 1):\n",
        "            # Bidirectional edges with distance features\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "            dist = np.linalg.norm(coords[i+1] - coords[i])\n",
        "            edge_attr.extend([\n",
        "                [dist, 1.0],  # Forward edge\n",
        "                [dist, 0.0]   # Backward edge\n",
        "            ])\n",
        "        return (torch.tensor(edge_index).t().contiguous(),\n",
        "                torch.tensor(np.array(edge_attr), dtype=torch.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced GNN Model Architecture\n",
        "# ----------------------------\n",
        "class RNA3DModel(nn.Module):\n",
        "    def __init__(self, in_channels=5, hidden_channels=512, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Enhanced input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Deeper GCN layers with residual connections\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.res_proj = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "            self.res_proj.append(nn.Linear(hidden_channels, hidden_channels) if i > 0 else None)\n",
        "\n",
        "        # Enhanced output module\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels*2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_channels*2, hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(hidden_channels, 3)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        # Initial projection\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Message passing with residual connections\n",
        "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
        "            res = x if self.res_proj[i] is None else self.res_proj[i](x)\n",
        "            x = conv(x, edge_index) + res\n",
        "            x = bn(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return self.output_net(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Optimized Training Utilities\n",
        "# ----------------------------\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, grad_accum=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, data in enumerate(loader):\n",
        "        data = data.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred, data.y) / grad_accum\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % grad_accum == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Tighter gradient clipping\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * grad_accum\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            total_loss += F.mse_loss(pred, data.y).item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced Main Training Loop\n",
        "# ----------------------------\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'train_sequences': '/kaggle/cleaned_train_sequences2 (1).csv',\n",
        "        'train_labels': '/kaggle/train_labels1.csv',\n",
        "        'validation_sequences': '/kaggle/validation_sequences.csv',\n",
        "        'validation_labels': '/kaggle/validation_labels.csv',\n",
        "        'batch_size': 32 if torch.cuda.is_available() else 8,\n",
        "        'hidden_dim': 512,\n",
        "        'num_layers': 6,\n",
        "        'max_epochs': 150,\n",
        "        'patience': 25,\n",
        "        'min_delta': 0.0005,\n",
        "        'grad_accum': 4\n",
        "    }\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if str(device) == 'cpu':\n",
        "        print(\"WARNING: Training on CPU will be slow! Consider using GPU acceleration\")\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = RNAGraphDataset(config['train_sequences'], config['train_labels'])\n",
        "    val_dataset = RNAGraphDataset(config['validation_sequences'], config['validation_labels'])\n",
        "\n",
        "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "        raise ValueError(\"No valid training/validation data found!\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=4 if str(device) == 'cuda' else 0\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=4 if str(device) == 'cuda' else 0\n",
        "    )\n",
        "\n",
        "    # Model setup\n",
        "    model = RNA3DModel(\n",
        "        hidden_channels=config['hidden_dim'],\n",
        "        num_layers=config['num_layers']\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=1e-3,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=config['max_epochs'],\n",
        "        pct_start=0.3,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val = float('inf')\n",
        "    patience = config['patience']\n",
        "\n",
        "    for epoch in range(1, config['max_epochs'] + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device, config['grad_accum'])\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | \"\n",
        "              f\"Train Loss: {train_loss:.6f} | \"\n",
        "              f\"Val Loss: {val_loss:.6f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # Enhanced early stopping\n",
        "        if val_loss < (best_val - config['min_delta']):\n",
        "            best_val = val_loss\n",
        "            patience = config['patience']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': val_loss,\n",
        "            }, \"best_model.pt\")\n",
        "            print(f\"New best model saved (Val Loss: {val_loss:.6f})\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
        "                print(f\"Best validation loss achieved: {best_val:.6f}\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iU8GSqf3D7s",
        "outputId": "ea4e5870-4bcb-4a2a-add3-b3ef95be823c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "WARNING: Training on CPU will be slow! Consider using GPU acceleration\n",
            "Model parameters: 3,950,083\n",
            "Epoch 001 | Train Loss: 157.879368 | Val Loss: 7.729547 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 7.729547)\n",
            "Epoch 002 | Train Loss: 89.482609 | Val Loss: 12.405952 | LR: 4.00e-05\n",
            "Epoch 003 | Train Loss: 58.264524 | Val Loss: 9.015454 | LR: 4.00e-05\n",
            "Epoch 004 | Train Loss: 40.596371 | Val Loss: 5.506173 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 5.506173)\n",
            "Epoch 005 | Train Loss: 29.989074 | Val Loss: 4.281924 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 4.281924)\n",
            "Epoch 006 | Train Loss: 22.758186 | Val Loss: 2.943737 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 2.943737)\n",
            "Epoch 007 | Train Loss: 17.732665 | Val Loss: 2.280376 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 2.280376)\n",
            "Epoch 008 | Train Loss: 14.436116 | Val Loss: 1.674589 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 1.674589)\n",
            "Epoch 009 | Train Loss: 11.752934 | Val Loss: 1.310864 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 1.310864)\n",
            "Epoch 010 | Train Loss: 9.619386 | Val Loss: 1.363136 | LR: 4.00e-05\n",
            "Epoch 011 | Train Loss: 8.144333 | Val Loss: 1.129925 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 1.129925)\n",
            "Epoch 012 | Train Loss: 6.740791 | Val Loss: 0.991112 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.991112)\n",
            "Epoch 013 | Train Loss: 5.817129 | Val Loss: 0.980678 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.980678)\n",
            "Epoch 014 | Train Loss: 4.963000 | Val Loss: 0.883308 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.883308)\n",
            "Epoch 015 | Train Loss: 4.331331 | Val Loss: 0.883082 | LR: 4.00e-05\n",
            "Epoch 016 | Train Loss: 3.788732 | Val Loss: 0.853510 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.853510)\n",
            "Epoch 017 | Train Loss: 3.267244 | Val Loss: 0.767521 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.767521)\n",
            "Epoch 018 | Train Loss: 2.999068 | Val Loss: 0.739868 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.739868)\n",
            "Epoch 019 | Train Loss: 2.720950 | Val Loss: 0.731856 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.731856)\n",
            "Epoch 020 | Train Loss: 2.552711 | Val Loss: 0.730673 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.730673)\n",
            "Epoch 021 | Train Loss: 2.326134 | Val Loss: 0.711272 | LR: 4.00e-05\n",
            "New best model saved (Val Loss: 0.711272)\n",
            "Epoch 022 | Train Loss: 2.101997 | Val Loss: 0.730296 | LR: 4.01e-05\n",
            "Epoch 023 | Train Loss: 2.082427 | Val Loss: 0.765606 | LR: 4.01e-05\n",
            "Epoch 024 | Train Loss: 1.937901 | Val Loss: 0.716901 | LR: 4.01e-05\n",
            "Epoch 025 | Train Loss: 1.763877 | Val Loss: 0.674800 | LR: 4.01e-05\n",
            "New best model saved (Val Loss: 0.674800)\n",
            "Epoch 026 | Train Loss: 1.703513 | Val Loss: 0.688597 | LR: 4.01e-05\n",
            "Epoch 027 | Train Loss: 1.530736 | Val Loss: 0.705822 | LR: 4.01e-05\n",
            "Epoch 028 | Train Loss: 1.531138 | Val Loss: 0.674289 | LR: 4.01e-05\n",
            "New best model saved (Val Loss: 0.674289)\n",
            "Epoch 029 | Train Loss: 1.491679 | Val Loss: 0.720329 | LR: 4.01e-05\n",
            "Epoch 030 | Train Loss: 1.417430 | Val Loss: 0.687319 | LR: 4.01e-05\n",
            "Epoch 031 | Train Loss: 1.418775 | Val Loss: 0.692436 | LR: 4.01e-05\n",
            "Epoch 032 | Train Loss: 1.373520 | Val Loss: 0.705963 | LR: 4.01e-05\n",
            "Epoch 033 | Train Loss: 1.355816 | Val Loss: 0.678094 | LR: 4.01e-05\n",
            "Epoch 034 | Train Loss: 1.266186 | Val Loss: 0.664252 | LR: 4.01e-05\n",
            "New best model saved (Val Loss: 0.664252)\n",
            "Epoch 035 | Train Loss: 1.253692 | Val Loss: 0.675636 | LR: 4.01e-05\n",
            "Epoch 036 | Train Loss: 1.258260 | Val Loss: 0.670255 | LR: 4.01e-05\n",
            "Epoch 037 | Train Loss: 1.183147 | Val Loss: 0.647518 | LR: 4.01e-05\n",
            "New best model saved (Val Loss: 0.647518)\n",
            "Epoch 038 | Train Loss: 1.191728 | Val Loss: 0.675796 | LR: 4.02e-05\n",
            "Epoch 039 | Train Loss: 1.198672 | Val Loss: 0.647568 | LR: 4.02e-05\n",
            "Epoch 040 | Train Loss: 1.190791 | Val Loss: 0.658080 | LR: 4.02e-05\n",
            "Epoch 041 | Train Loss: 1.165070 | Val Loss: 0.646312 | LR: 4.02e-05\n",
            "New best model saved (Val Loss: 0.646312)\n",
            "Epoch 042 | Train Loss: 1.148037 | Val Loss: 0.637261 | LR: 4.02e-05\n",
            "New best model saved (Val Loss: 0.637261)\n",
            "Epoch 043 | Train Loss: 1.137833 | Val Loss: 0.640611 | LR: 4.02e-05\n",
            "Epoch 044 | Train Loss: 1.087864 | Val Loss: 0.653662 | LR: 4.02e-05\n",
            "Epoch 045 | Train Loss: 1.040562 | Val Loss: 0.635121 | LR: 4.02e-05\n",
            "New best model saved (Val Loss: 0.635121)\n",
            "Epoch 046 | Train Loss: 1.117052 | Val Loss: 0.706962 | LR: 4.02e-05\n",
            "Epoch 047 | Train Loss: 1.070403 | Val Loss: 0.667792 | LR: 4.02e-05\n",
            "Epoch 048 | Train Loss: 1.054657 | Val Loss: 0.672119 | LR: 4.02e-05\n",
            "Epoch 049 | Train Loss: 0.999942 | Val Loss: 0.641419 | LR: 4.03e-05\n",
            "Epoch 050 | Train Loss: 1.007814 | Val Loss: 0.636221 | LR: 4.03e-05\n",
            "Epoch 051 | Train Loss: 1.018139 | Val Loss: 0.673481 | LR: 4.03e-05\n",
            "Epoch 052 | Train Loss: 0.987521 | Val Loss: 0.675656 | LR: 4.03e-05\n",
            "Epoch 053 | Train Loss: 1.013963 | Val Loss: 0.663260 | LR: 4.03e-05\n",
            "Epoch 054 | Train Loss: 1.016145 | Val Loss: 0.646918 | LR: 4.03e-05\n",
            "Epoch 055 | Train Loss: 0.958494 | Val Loss: 0.641998 | LR: 4.03e-05\n",
            "Epoch 056 | Train Loss: 0.990722 | Val Loss: 0.650925 | LR: 4.03e-05\n",
            "Epoch 057 | Train Loss: 0.995256 | Val Loss: 0.644152 | LR: 4.03e-05\n",
            "Epoch 058 | Train Loss: 0.913737 | Val Loss: 0.645282 | LR: 4.04e-05\n",
            "Epoch 059 | Train Loss: 0.883471 | Val Loss: 0.639261 | LR: 4.04e-05\n",
            "Epoch 060 | Train Loss: 0.902774 | Val Loss: 0.644787 | LR: 4.04e-05\n",
            "Epoch 061 | Train Loss: 0.882106 | Val Loss: 0.673556 | LR: 4.04e-05\n",
            "Epoch 062 | Train Loss: 0.907803 | Val Loss: 0.646294 | LR: 4.04e-05\n",
            "Epoch 063 | Train Loss: 0.854045 | Val Loss: 0.643599 | LR: 4.04e-05\n",
            "Epoch 064 | Train Loss: 0.922451 | Val Loss: 0.634465 | LR: 4.04e-05\n",
            "New best model saved (Val Loss: 0.634465)\n",
            "Epoch 065 | Train Loss: 0.983377 | Val Loss: 0.665192 | LR: 4.04e-05\n",
            "Epoch 066 | Train Loss: 0.963073 | Val Loss: 0.689916 | LR: 4.05e-05\n",
            "Epoch 067 | Train Loss: 0.943271 | Val Loss: 0.682842 | LR: 4.05e-05\n",
            "Epoch 068 | Train Loss: 0.903718 | Val Loss: 0.670467 | LR: 4.05e-05\n",
            "Epoch 069 | Train Loss: 0.887958 | Val Loss: 0.679327 | LR: 4.05e-05\n",
            "Epoch 070 | Train Loss: 0.929336 | Val Loss: 0.648882 | LR: 4.05e-05\n",
            "Epoch 071 | Train Loss: 0.928443 | Val Loss: 0.662732 | LR: 4.05e-05\n",
            "Epoch 072 | Train Loss: 0.931205 | Val Loss: 0.642483 | LR: 4.05e-05\n",
            "Epoch 073 | Train Loss: 0.869595 | Val Loss: 0.642488 | LR: 4.06e-05\n",
            "Epoch 074 | Train Loss: 0.893531 | Val Loss: 0.697227 | LR: 4.06e-05\n",
            "Epoch 075 | Train Loss: 0.896629 | Val Loss: 0.651264 | LR: 4.06e-05\n",
            "Epoch 076 | Train Loss: 0.903291 | Val Loss: 0.666387 | LR: 4.06e-05\n",
            "Epoch 077 | Train Loss: 0.933324 | Val Loss: 0.685022 | LR: 4.06e-05\n",
            "Epoch 078 | Train Loss: 0.907042 | Val Loss: 0.645643 | LR: 4.06e-05\n",
            "Epoch 079 | Train Loss: 0.822256 | Val Loss: 0.726628 | LR: 4.06e-05\n",
            "Epoch 080 | Train Loss: 0.892887 | Val Loss: 0.638957 | LR: 4.07e-05\n",
            "Epoch 081 | Train Loss: 0.907992 | Val Loss: 0.658453 | LR: 4.07e-05\n",
            "Epoch 082 | Train Loss: 0.940806 | Val Loss: 0.726824 | LR: 4.07e-05\n",
            "Epoch 083 | Train Loss: 0.905907 | Val Loss: 0.636117 | LR: 4.07e-05\n",
            "Epoch 084 | Train Loss: 0.879014 | Val Loss: 0.695081 | LR: 4.07e-05\n",
            "Epoch 085 | Train Loss: 0.887845 | Val Loss: 0.656620 | LR: 4.08e-05\n",
            "Epoch 086 | Train Loss: 0.895352 | Val Loss: 0.635263 | LR: 4.08e-05\n",
            "Epoch 087 | Train Loss: 0.806327 | Val Loss: 0.657272 | LR: 4.08e-05\n",
            "Epoch 088 | Train Loss: 0.940000 | Val Loss: 0.643551 | LR: 4.08e-05\n",
            "Epoch 089 | Train Loss: 0.813245 | Val Loss: 0.656516 | LR: 4.08e-05\n",
            "\n",
            "Early stopping triggered at epoch 89\n",
            "Best validation loss achieved: 0.634465\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import warnings\n",
        "from torch.nn import MultiheadAttention\n",
        "\n",
        "# Suppress unnecessary warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced Dataset with Geometric Features\n",
        "# ----------------------------\n",
        "class RNAGraphDataset(Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.graphs = self._process_data()\n",
        "\n",
        "    def _process_data(self):\n",
        "        graphs = []\n",
        "        all_coords = []\n",
        "\n",
        "        # First pass: collect statistics\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if not matched_labels.empty and len(matched_labels) == len(row['sequence']):\n",
        "                coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "                all_coords.append(coords)\n",
        "\n",
        "        all_coords = np.concatenate(all_coords, axis=0)\n",
        "        self.mean = all_coords.mean(axis=0)\n",
        "        self.std = all_coords.std(axis=0) + 1e-8\n",
        "\n",
        "        # Second pass: build graphs\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "\n",
        "            node_feats = self._encode_sequence(sequence)\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "\n",
        "            edge_index, edge_attr = self._build_edges(len(sequence), coords)\n",
        "\n",
        "            graphs.append(Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float32),\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                y=torch.tensor(coords, dtype=torch.float32),\n",
        "                pos=torch.tensor(coords, dtype=torch.float32)\n",
        "            ))\n",
        "        return graphs\n",
        "\n",
        "    def _encode_sequence(self, sequence):\n",
        "        mapping = {\n",
        "            'A': [1,0,0,0, 0.12, 0.89],\n",
        "            'U': [0,1,0,0, 0.23, 0.76],\n",
        "            'G': [0,0,1,0, 0.34, 0.65],\n",
        "            'C': [0,0,0,1, 0.45, 0.54]\n",
        "        }\n",
        "        return [mapping.get(nt, [0]*6) for nt in sequence]\n",
        "\n",
        "    def _build_edges(self, length, coords):\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        for i in range(length - 1):\n",
        "            vec = coords[i+1] - coords[i]\n",
        "            dist = np.linalg.norm(vec)\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "            edge_attr.extend([\n",
        "                [dist, vec[0], vec[1], vec[2], 1.0],\n",
        "                [dist, -vec[0], -vec[1], -vec[2], 0.0]\n",
        "            ])\n",
        "        return (torch.tensor(edge_index).t().contiguous(),\n",
        "                torch.tensor(np.array(edge_attr), dtype=torch.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced GNN Model\n",
        "# ----------------------------\n",
        "class RNA3DModel(nn.Module):\n",
        "    def __init__(self, in_channels=6, hidden_channels=512, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels, improved=True))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.attn = MultiheadAttention(hidden_channels, num_heads=4, dropout=0.1)\n",
        "        self.attn_ln = nn.LayerNorm(hidden_channels)\n",
        "\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels*2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_channels*2, hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(hidden_channels, 3)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index) + x\n",
        "            x = bn(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        attn_out, _ = self.attn(x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0))\n",
        "        x = x + self.attn_ln(attn_out.squeeze(0))\n",
        "\n",
        "        return self.output_net(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Utilities\n",
        "# ----------------------------\n",
        "def train_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred, data.y)\n",
        "\n",
        "        # Regularization\n",
        "        l2_reg = 0.001 * sum(p.norm(2) for p in model.parameters())\n",
        "        l1_reg = 0.0001 * sum(p.abs().sum() for p in model.parameters())\n",
        "        loss += l2_reg + l1_reg\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            total_loss += F.mse_loss(pred, data.y).item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training Loop\n",
        "# ----------------------------\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"train_sequences\": \"/kaggle/cleaned_train_sequences2 (1).csv\",\n",
        "        \"train_labels\": \"/kaggle/train_labels1.csv\",\n",
        "        \"validation_sequences\": \"/kaggle/validation_sequences.csv\",\n",
        "        \"validation_labels\": \"/kaggle/validation_labels.csv\",\n",
        "        \"batch_size\": 32 if torch.cuda.is_available() else 8,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"num_layers\": 6,\n",
        "        \"max_epochs\": 150,\n",
        "        \"patience\": 25,\n",
        "        \"min_delta\": 0.0005\n",
        "    }\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = RNAGraphDataset(config[\"train_sequences\"], config[\"train_labels\"])\n",
        "    val_dataset = RNAGraphDataset(config[\"validation_sequences\"], config[\"validation_labels\"])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Model setup\n",
        "    model = RNA3DModel(\n",
        "        hidden_channels=config[\"hidden_dim\"],\n",
        "        num_layers=config[\"num_layers\"]\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=2e-4,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=config[\"max_epochs\"],\n",
        "        pct_start=0.4,\n",
        "        div_factor=5,\n",
        "        final_div_factor=1000\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val = float('inf')\n",
        "    patience = config[\"patience\"]\n",
        "\n",
        "    for epoch in range(1, config[\"max_epochs\"] + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < (best_val - config[\"min_delta\"]):\n",
        "            best_val = val_loss\n",
        "            patience = config[\"patience\"]\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': val_loss,\n",
        "            }, \"best_model.pt\")\n",
        "            print(f\"New best model saved (Val Loss: {val_loss:.6f})\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
        "                print(f\"Best validation loss achieved: {best_val:.6f}\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhQplBWP-cFq",
        "outputId": "5d1572f0-59d1-430d-f3fb-183e2e63b1af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model parameters: 3,688,963\n",
            "Epoch 001 | Train Loss: 157.154622 | Val Loss: 13.349664\n",
            "New best model saved (Val Loss: 13.349664)\n",
            "Epoch 002 | Train Loss: 37.812583 | Val Loss: 3.356622\n",
            "New best model saved (Val Loss: 3.356622)\n",
            "Epoch 003 | Train Loss: 20.511656 | Val Loss: 1.159717\n",
            "New best model saved (Val Loss: 1.159717)\n",
            "Epoch 004 | Train Loss: 16.391667 | Val Loss: 0.760597\n",
            "New best model saved (Val Loss: 0.760597)\n",
            "Epoch 005 | Train Loss: 15.212419 | Val Loss: 0.737906\n",
            "New best model saved (Val Loss: 0.737906)\n",
            "Epoch 006 | Train Loss: 14.774716 | Val Loss: 0.864384\n",
            "Epoch 007 | Train Loss: 14.471766 | Val Loss: 0.721812\n",
            "New best model saved (Val Loss: 0.721812)\n",
            "Epoch 008 | Train Loss: 14.251153 | Val Loss: 0.654009\n",
            "New best model saved (Val Loss: 0.654009)\n",
            "Epoch 009 | Train Loss: 14.152201 | Val Loss: 0.659165\n",
            "Epoch 010 | Train Loss: 14.069575 | Val Loss: 0.735835\n",
            "Epoch 011 | Train Loss: 13.982658 | Val Loss: 0.934914\n",
            "Epoch 012 | Train Loss: 13.817139 | Val Loss: 0.665654\n",
            "Epoch 013 | Train Loss: 13.823338 | Val Loss: 0.704934\n",
            "Epoch 014 | Train Loss: 13.489571 | Val Loss: 0.668966\n",
            "Epoch 015 | Train Loss: 13.279098 | Val Loss: 0.635060\n",
            "New best model saved (Val Loss: 0.635060)\n",
            "Epoch 016 | Train Loss: 13.177137 | Val Loss: 0.684097\n",
            "Epoch 017 | Train Loss: 12.900338 | Val Loss: 0.635883\n",
            "Epoch 018 | Train Loss: 12.708042 | Val Loss: 0.656708\n",
            "Epoch 019 | Train Loss: 12.576425 | Val Loss: 0.641362\n",
            "Epoch 020 | Train Loss: 12.365273 | Val Loss: 0.681994\n",
            "Epoch 021 | Train Loss: 12.075175 | Val Loss: 0.642854\n",
            "Epoch 022 | Train Loss: 11.871266 | Val Loss: 0.704808\n",
            "Epoch 023 | Train Loss: 11.676183 | Val Loss: 0.749174\n",
            "Epoch 024 | Train Loss: 11.413096 | Val Loss: 0.686586\n",
            "Epoch 025 | Train Loss: 11.087487 | Val Loss: 0.662032\n",
            "Epoch 026 | Train Loss: 10.908590 | Val Loss: 0.733742\n",
            "Epoch 027 | Train Loss: 10.665459 | Val Loss: 0.696499\n",
            "Epoch 028 | Train Loss: 10.393219 | Val Loss: 0.726997\n",
            "Epoch 029 | Train Loss: 10.242389 | Val Loss: 0.692220\n",
            "Epoch 030 | Train Loss: 9.901488 | Val Loss: 0.632976\n",
            "New best model saved (Val Loss: 0.632976)\n",
            "Epoch 031 | Train Loss: 9.677761 | Val Loss: 0.652641\n",
            "Epoch 032 | Train Loss: 9.479400 | Val Loss: 0.659021\n",
            "Epoch 033 | Train Loss: 9.145008 | Val Loss: 0.658500\n",
            "Epoch 034 | Train Loss: 9.051613 | Val Loss: 0.647260\n",
            "Epoch 035 | Train Loss: 8.814991 | Val Loss: 0.632429\n",
            "New best model saved (Val Loss: 0.632429)\n",
            "Epoch 036 | Train Loss: 8.584291 | Val Loss: 0.654868\n",
            "Epoch 037 | Train Loss: 8.370668 | Val Loss: 0.629196\n",
            "New best model saved (Val Loss: 0.629196)\n",
            "Epoch 038 | Train Loss: 8.169652 | Val Loss: 0.650924\n",
            "Epoch 039 | Train Loss: 7.970613 | Val Loss: 0.677762\n",
            "Epoch 040 | Train Loss: 7.754254 | Val Loss: 0.647601\n",
            "Epoch 041 | Train Loss: 7.607964 | Val Loss: 0.638032\n",
            "Epoch 042 | Train Loss: 7.400071 | Val Loss: 0.635001\n",
            "Epoch 043 | Train Loss: 7.316430 | Val Loss: 0.699066\n",
            "Epoch 044 | Train Loss: 7.075480 | Val Loss: 0.646732\n",
            "Epoch 045 | Train Loss: 6.982464 | Val Loss: 0.661038\n",
            "Epoch 046 | Train Loss: 6.858986 | Val Loss: 0.742249\n",
            "Epoch 047 | Train Loss: 6.659718 | Val Loss: 0.638824\n",
            "Epoch 048 | Train Loss: 6.603702 | Val Loss: 0.634090\n",
            "Epoch 049 | Train Loss: 6.492439 | Val Loss: 0.690851\n",
            "Epoch 050 | Train Loss: 6.387911 | Val Loss: 0.650254\n",
            "Epoch 051 | Train Loss: 6.287586 | Val Loss: 0.675840\n",
            "Epoch 052 | Train Loss: 6.108929 | Val Loss: 0.645161\n",
            "Epoch 053 | Train Loss: 5.996903 | Val Loss: 0.641121\n",
            "Epoch 054 | Train Loss: 6.000284 | Val Loss: 0.717449\n",
            "Epoch 055 | Train Loss: 5.919766 | Val Loss: 0.660503\n",
            "Epoch 056 | Train Loss: 5.806801 | Val Loss: 0.704708\n",
            "Epoch 057 | Train Loss: 5.704141 | Val Loss: 0.706074\n",
            "Epoch 058 | Train Loss: 5.646584 | Val Loss: 0.714063\n",
            "Epoch 059 | Train Loss: 5.570157 | Val Loss: 0.638894\n",
            "Epoch 060 | Train Loss: 5.477115 | Val Loss: 0.653268\n",
            "Epoch 061 | Train Loss: 5.447594 | Val Loss: 0.636979\n",
            "Epoch 062 | Train Loss: 5.364546 | Val Loss: 0.679501\n",
            "\n",
            "Early stopping triggered at epoch 62\n",
            "Best validation loss achieved: 0.629196\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import pandas as pd\n",
        "\n",
        "# Enhanced Dataset with Augmentations\n",
        "class RNAGraphDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences_file, labels_file):\n",
        "        self.sequences = pd.read_csv(sequences_file)\n",
        "        self.labels = pd.read_csv(labels_file)\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.graphs = self._process_data()\n",
        "        self.lengths = [len(seq) for seq in self.sequences['sequence']]\n",
        "        self.max_length = max(self.lengths) if self.lengths else 1\n",
        "\n",
        "    def _process_data(self):\n",
        "        graphs = []\n",
        "        all_coords = []\n",
        "\n",
        "        # First pass: collect statistics\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "            if not matched_labels.empty and len(matched_labels) == len(row['sequence']):\n",
        "                coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "                all_coords.append(coords)\n",
        "\n",
        "        all_coords = np.concatenate(all_coords, axis=0) if all_coords else np.zeros((1,3))\n",
        "        self.mean = all_coords.mean(axis=0)\n",
        "        self.std = all_coords.std(axis=0) + 1e-8\n",
        "\n",
        "        # Second pass: build graphs\n",
        "        for _, row in self.sequences.iterrows():\n",
        "            target_id = row['target_id']\n",
        "            sequence = row['sequence']\n",
        "            matched_labels = self.labels[self.labels['ID'].str.startswith(target_id)]\n",
        "\n",
        "            if matched_labels.empty or len(matched_labels) != len(sequence):\n",
        "                continue\n",
        "\n",
        "            node_feats = self._encode_sequence(sequence)\n",
        "            coords = matched_labels[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n",
        "            coords = (coords - self.mean) / self.std\n",
        "\n",
        "            edge_index, edge_attr = self._build_edges(len(sequence), coords)\n",
        "\n",
        "            graphs.append(Data(\n",
        "                x=torch.tensor(node_feats, dtype=torch.float32),\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                y=torch.tensor(coords, dtype=torch.float32),\n",
        "                pos=torch.tensor(coords, dtype=torch.float32)\n",
        "            ))\n",
        "        return graphs\n",
        "\n",
        "    def _encode_sequence(self, sequence):\n",
        "        mapping = {\n",
        "            'A': [1,0,0,0, 0.12, 0.89],\n",
        "            'U': [0,1,0,0, 0.23, 0.76],\n",
        "            'G': [0,0,1,0, 0.34, 0.65],\n",
        "            'C': [0,0,0,1, 0.45, 0.54]\n",
        "        }\n",
        "        return [mapping.get(nt, [0]*6) for nt in sequence]\n",
        "\n",
        "    def _build_edges(self, length, coords):\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        for i in range(length - 1):\n",
        "            vec = coords[i+1] - coords[i]\n",
        "            dist = np.linalg.norm(vec)\n",
        "            edge_index.append([i, i+1])\n",
        "            edge_index.append([i+1, i])\n",
        "            edge_attr.extend([\n",
        "                [dist, vec[0], vec[1], vec[2], 1.0],\n",
        "                [dist, -vec[0], -vec[1], -vec[2], 0.0]\n",
        "            ])\n",
        "        return (torch.tensor(edge_index).t().contiguous(),\n",
        "                torch.tensor(np.array(edge_attr), dtype=torch.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.graphs[idx]\n",
        "\n",
        "        # Enhanced augmentations\n",
        "        if torch.rand(1) < 0.5:  # 50% augmentation probability\n",
        "            noise_level = 0.02 * (self.lengths[idx]/self.max_length)\n",
        "            data.y += torch.randn_like(data.y) * noise_level\n",
        "            data.pos += torch.randn_like(data.pos) * noise_level\n",
        "            if hasattr(data, 'edge_attr'):\n",
        "                data.edge_attr[:, :4] += torch.randn_like(data.edge_attr[:, :4]) * (noise_level/2)\n",
        "\n",
        "        return data\n",
        "\n",
        "# Enhanced Model Architecture\n",
        "class RNA3DModel(nn.Module):\n",
        "    def __init__(self, in_channels=6, hidden_channels=512, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection with layer norm\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.LayerNorm(hidden_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # GCN layers with residual connections\n",
        "        self.convs = nn.ModuleList([\n",
        "            GCNConv(hidden_channels, hidden_channels, improved=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.bns = nn.ModuleList([\n",
        "            nn.BatchNorm1d(hidden_channels)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output network with skip connection\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels*2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_channels*2, 3)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Input projection\n",
        "        x = self.input_proj(x)\n",
        "        x_skip = x  # Save for skip connection\n",
        "\n",
        "        # Message passing\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index) + x  # Residual\n",
        "            x = bn(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        # Output with skip connection\n",
        "        return self.output_net(x + x_skip)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            pred = model(data)\n",
        "            total_loss += F.mse_loss(pred, data.y).item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"train_sequences\": \"/kaggle/cleaned_train_sequences2 (1).csv\",\n",
        "        \"train_labels\": \"/kaggle/train_labels1.csv\",\n",
        "        \"validation_sequences\": \"/kaggle/validation_sequences.csv\",\n",
        "        \"validation_labels\": \"/kaggle/validation_labels.csv\",\n",
        "        \"batch_size\": 32 if torch.cuda.is_available() else 8,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"num_layers\": 6,\n",
        "        \"max_epochs\": 200,\n",
        "        \"patience\": 30,\n",
        "        \"min_delta\": 0.0001,\n",
        "        \"grad_clip\": 0.3\n",
        "    }\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = RNAGraphDataset(config[\"train_sequences\"], config[\"train_labels\"])\n",
        "    val_dataset = RNAGraphDataset(config[\"validation_sequences\"], config[\"validation_labels\"])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        collate_fn=lambda x: Batch.from_data_list(x),\n",
        "        num_workers=4 if str(device) == 'cuda' else 0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Model setup\n",
        "    model = RNA3DModel(\n",
        "        hidden_channels=config[\"hidden_dim\"],\n",
        "        num_layers=config[\"num_layers\"]\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=1e-5\n",
        "    )\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=3e-4,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=config[\"max_epochs\"],\n",
        "        pct_start=0.3,\n",
        "        div_factor=10,\n",
        "        final_div_factor=1000\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    best_val = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(1, config[\"max_epochs\"] + 1):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(data)\n",
        "            loss = F.mse_loss(pred, data.y)\n",
        "\n",
        "            # Enhanced regularization\n",
        "            loss += 0.01 * sum(p.norm(2) for p in model.parameters())\n",
        "            loss += 0.001 * sum(p.abs().sum() for p in model.parameters())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch:03d} | \"\n",
        "              f\"Train Loss: {train_loss/len(train_loader):.6f} | \"\n",
        "              f\"Val Loss: {val_loss:.6f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val - config[\"min_delta\"]:\n",
        "            best_val = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            print(f\"New best model saved (Val Loss: {val_loss:.6f})\")\n",
        "\n",
        "        if epoch - best_epoch > config[\"patience\"]:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
        "            print(f\"Best validation loss achieved: {best_val:.6f}\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIaSkxVlDKSq",
        "outputId": "f7da0991-e56e-4c47-a1f1-24baf762c4ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model parameters: 2,115,075\n",
            "Epoch 001 | Train Loss: 288.483676 | Val Loss: 9.988841 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 9.988841)\n",
            "Epoch 002 | Train Loss: 174.345858 | Val Loss: 5.012448 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 5.012448)\n",
            "Epoch 003 | Train Loss: 126.142201 | Val Loss: 2.365739 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 2.365739)\n",
            "Epoch 004 | Train Loss: 105.116877 | Val Loss: 1.538714 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 1.538714)\n",
            "Epoch 005 | Train Loss: 96.516326 | Val Loss: 1.024414 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 1.024414)\n",
            "Epoch 006 | Train Loss: 92.858419 | Val Loss: 0.860663 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.860663)\n",
            "Epoch 007 | Train Loss: 90.685149 | Val Loss: 0.867502 | LR: 3.00e-05\n",
            "Epoch 008 | Train Loss: 89.176761 | Val Loss: 0.730641 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.730641)\n",
            "Epoch 009 | Train Loss: 87.723397 | Val Loss: 0.837185 | LR: 3.00e-05\n",
            "Epoch 010 | Train Loss: 86.295821 | Val Loss: 0.679019 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.679019)\n",
            "Epoch 011 | Train Loss: 84.860241 | Val Loss: 0.768108 | LR: 3.00e-05\n",
            "Epoch 012 | Train Loss: 83.216688 | Val Loss: 0.839726 | LR: 3.00e-05\n",
            "Epoch 013 | Train Loss: 81.727317 | Val Loss: 0.795648 | LR: 3.00e-05\n",
            "Epoch 014 | Train Loss: 80.101143 | Val Loss: 0.741804 | LR: 3.00e-05\n",
            "Epoch 015 | Train Loss: 78.352632 | Val Loss: 0.774213 | LR: 3.00e-05\n",
            "Epoch 016 | Train Loss: 76.566361 | Val Loss: 0.761578 | LR: 3.00e-05\n",
            "Epoch 017 | Train Loss: 74.693453 | Val Loss: 0.756013 | LR: 3.00e-05\n",
            "Epoch 018 | Train Loss: 72.781891 | Val Loss: 0.661680 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.661680)\n",
            "Epoch 019 | Train Loss: 70.924689 | Val Loss: 0.680793 | LR: 3.00e-05\n",
            "Epoch 020 | Train Loss: 69.020340 | Val Loss: 0.691873 | LR: 3.00e-05\n",
            "Epoch 021 | Train Loss: 66.935072 | Val Loss: 0.877097 | LR: 3.00e-05\n",
            "Epoch 022 | Train Loss: 65.371215 | Val Loss: 0.813828 | LR: 3.00e-05\n",
            "Epoch 023 | Train Loss: 63.377313 | Val Loss: 0.708668 | LR: 3.00e-05\n",
            "Epoch 024 | Train Loss: 61.548811 | Val Loss: 0.710282 | LR: 3.00e-05\n",
            "Epoch 025 | Train Loss: 59.803183 | Val Loss: 0.707087 | LR: 3.00e-05\n",
            "Epoch 026 | Train Loss: 57.850434 | Val Loss: 0.677510 | LR: 3.00e-05\n",
            "Epoch 027 | Train Loss: 56.094457 | Val Loss: 0.726977 | LR: 3.00e-05\n",
            "Epoch 028 | Train Loss: 54.423077 | Val Loss: 0.772314 | LR: 3.00e-05\n",
            "Epoch 029 | Train Loss: 52.648044 | Val Loss: 0.868390 | LR: 3.00e-05\n",
            "Epoch 030 | Train Loss: 51.004738 | Val Loss: 0.671390 | LR: 3.00e-05\n",
            "Epoch 031 | Train Loss: 49.169196 | Val Loss: 0.702746 | LR: 3.00e-05\n",
            "Epoch 032 | Train Loss: 47.481279 | Val Loss: 0.809359 | LR: 3.00e-05\n",
            "Epoch 033 | Train Loss: 45.959076 | Val Loss: 0.718212 | LR: 3.00e-05\n",
            "Epoch 034 | Train Loss: 44.406016 | Val Loss: 0.684679 | LR: 3.00e-05\n",
            "Epoch 035 | Train Loss: 42.933477 | Val Loss: 0.819589 | LR: 3.00e-05\n",
            "Epoch 036 | Train Loss: 41.647447 | Val Loss: 0.642157 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.642157)\n",
            "Epoch 037 | Train Loss: 40.391081 | Val Loss: 0.641970 | LR: 3.00e-05\n",
            "New best model saved (Val Loss: 0.641970)\n",
            "Epoch 038 | Train Loss: 39.030848 | Val Loss: 0.671837 | LR: 3.00e-05\n",
            "Epoch 039 | Train Loss: 37.796561 | Val Loss: 0.657593 | LR: 3.00e-05\n",
            "Epoch 040 | Train Loss: 36.793626 | Val Loss: 0.782666 | LR: 3.00e-05\n",
            "Epoch 041 | Train Loss: 35.706278 | Val Loss: 0.667040 | LR: 3.00e-05\n",
            "Epoch 042 | Train Loss: 34.802039 | Val Loss: 0.667136 | LR: 3.00e-05\n",
            "Epoch 043 | Train Loss: 33.835615 | Val Loss: 0.748819 | LR: 3.00e-05\n",
            "Epoch 044 | Train Loss: 33.057989 | Val Loss: 0.779400 | LR: 3.00e-05\n",
            "Epoch 045 | Train Loss: 32.295086 | Val Loss: 0.648116 | LR: 3.00e-05\n",
            "Epoch 046 | Train Loss: 31.573442 | Val Loss: 0.678945 | LR: 3.00e-05\n",
            "Epoch 047 | Train Loss: 31.211151 | Val Loss: 0.666056 | LR: 3.00e-05\n",
            "Epoch 048 | Train Loss: 30.533705 | Val Loss: 0.656665 | LR: 3.00e-05\n",
            "Epoch 049 | Train Loss: 29.965821 | Val Loss: 0.718874 | LR: 3.00e-05\n",
            "Epoch 050 | Train Loss: 29.442884 | Val Loss: 0.819355 | LR: 3.00e-05\n",
            "Epoch 051 | Train Loss: 29.016805 | Val Loss: 0.675163 | LR: 3.00e-05\n",
            "Epoch 052 | Train Loss: 28.643430 | Val Loss: 0.691078 | LR: 3.00e-05\n",
            "Epoch 053 | Train Loss: 28.157018 | Val Loss: 0.664673 | LR: 3.00e-05\n",
            "Epoch 054 | Train Loss: 27.782350 | Val Loss: 0.694542 | LR: 3.00e-05\n",
            "Epoch 055 | Train Loss: 27.348502 | Val Loss: 0.696150 | LR: 3.00e-05\n",
            "Epoch 056 | Train Loss: 27.111475 | Val Loss: 0.653170 | LR: 3.01e-05\n",
            "Epoch 057 | Train Loss: 26.804521 | Val Loss: 0.680367 | LR: 3.01e-05\n",
            "Epoch 058 | Train Loss: 26.558074 | Val Loss: 0.684525 | LR: 3.01e-05\n",
            "Epoch 059 | Train Loss: 26.218743 | Val Loss: 0.647039 | LR: 3.01e-05\n",
            "Epoch 060 | Train Loss: 25.956767 | Val Loss: 0.741260 | LR: 3.01e-05\n",
            "Epoch 061 | Train Loss: 25.682763 | Val Loss: 0.640068 | LR: 3.01e-05\n",
            "New best model saved (Val Loss: 0.640068)\n",
            "Epoch 062 | Train Loss: 25.485505 | Val Loss: 0.674927 | LR: 3.01e-05\n",
            "Epoch 063 | Train Loss: 25.172912 | Val Loss: 0.710599 | LR: 3.01e-05\n",
            "Epoch 064 | Train Loss: 25.085891 | Val Loss: 0.701067 | LR: 3.01e-05\n",
            "Epoch 065 | Train Loss: 24.795699 | Val Loss: 0.643004 | LR: 3.01e-05\n",
            "Epoch 066 | Train Loss: 24.635223 | Val Loss: 0.714391 | LR: 3.01e-05\n",
            "Epoch 067 | Train Loss: 24.362306 | Val Loss: 0.676397 | LR: 3.01e-05\n",
            "Epoch 068 | Train Loss: 24.117598 | Val Loss: 0.663882 | LR: 3.01e-05\n",
            "Epoch 069 | Train Loss: 23.858150 | Val Loss: 0.657953 | LR: 3.01e-05\n",
            "Epoch 070 | Train Loss: 23.750212 | Val Loss: 0.674091 | LR: 3.01e-05\n",
            "Epoch 071 | Train Loss: 23.593573 | Val Loss: 0.692657 | LR: 3.01e-05\n",
            "Epoch 072 | Train Loss: 23.369671 | Val Loss: 0.717157 | LR: 3.01e-05\n",
            "Epoch 073 | Train Loss: 23.185069 | Val Loss: 0.737990 | LR: 3.01e-05\n",
            "Epoch 074 | Train Loss: 23.037337 | Val Loss: 0.666397 | LR: 3.01e-05\n",
            "Epoch 075 | Train Loss: 22.917460 | Val Loss: 0.744601 | LR: 3.01e-05\n",
            "Epoch 076 | Train Loss: 22.771708 | Val Loss: 0.762300 | LR: 3.01e-05\n",
            "Epoch 077 | Train Loss: 22.560513 | Val Loss: 0.655448 | LR: 3.01e-05\n",
            "Epoch 078 | Train Loss: 22.431560 | Val Loss: 0.651426 | LR: 3.01e-05\n",
            "Epoch 079 | Train Loss: 22.314306 | Val Loss: 0.668563 | LR: 3.01e-05\n",
            "Epoch 080 | Train Loss: 22.073935 | Val Loss: 0.668023 | LR: 3.01e-05\n",
            "Epoch 081 | Train Loss: 21.974622 | Val Loss: 0.771943 | LR: 3.01e-05\n",
            "Epoch 082 | Train Loss: 21.851411 | Val Loss: 0.684196 | LR: 3.01e-05\n",
            "Epoch 083 | Train Loss: 21.763999 | Val Loss: 0.719607 | LR: 3.01e-05\n",
            "Epoch 084 | Train Loss: 21.700027 | Val Loss: 0.741622 | LR: 3.01e-05\n",
            "Epoch 085 | Train Loss: 21.440102 | Val Loss: 0.680782 | LR: 3.01e-05\n",
            "Epoch 086 | Train Loss: 21.413483 | Val Loss: 0.693132 | LR: 3.01e-05\n",
            "Epoch 087 | Train Loss: 21.209313 | Val Loss: 0.649225 | LR: 3.01e-05\n",
            "Epoch 088 | Train Loss: 21.086521 | Val Loss: 0.831369 | LR: 3.01e-05\n",
            "Epoch 089 | Train Loss: 20.899704 | Val Loss: 0.783472 | LR: 3.01e-05\n",
            "Epoch 090 | Train Loss: 20.855777 | Val Loss: 0.671327 | LR: 3.01e-05\n",
            "Epoch 091 | Train Loss: 20.615053 | Val Loss: 0.676444 | LR: 3.01e-05\n",
            "Epoch 092 | Train Loss: 20.685175 | Val Loss: 0.699987 | LR: 3.01e-05\n",
            "\n",
            "Early stopping triggered at epoch 92\n",
            "Best validation loss achieved: 0.640068\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6qE6cthtda2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1913adba-d6a8-4284-ca8b-c96d831e47f6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define destination folder path in Google Drive\n",
        "drive_path = \"/content/drive/MyDrive/RNA_3D_Models/\"\n",
        "!mkdir -p \"$drive_path\"\n",
        "\n",
        "# Move the model\n",
        "shutil.copy(\"best_model.pt\", drive_path + \"best_model.pt\")\n",
        "print(f\"Model saved to {drive_path}best_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YPMeOGzKr27",
        "outputId": "b38ac9f9-3a59-44c4-ecf4-9b930a59ad70"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/RNA_3D_Models/best_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/RNA_3D_Models/best_model.pt\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dgzu1O2K07J",
        "outputId": "f95c920c-936b-4979-c0e3-7409850586db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNA3DModel(\n",
              "  (input_proj): Sequential(\n",
              "    (0): Linear(in_features=6, out_features=512, bias=True)\n",
              "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): LeakyReLU(negative_slope=0.1)\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (convs): ModuleList(\n",
              "    (0-5): 6 x GCNConv(512, 512)\n",
              "  )\n",
              "  (bns): ModuleList(\n",
              "    (0-5): 6 x BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (output_net): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.1)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), \"best_rna3d_model.pt\")\n"
      ],
      "metadata": {
        "id": "hf1RMQrWLCWz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder and save the model\n",
        "drive_path = \"/content/drive/MyDrive/RNA_3D_Models/\"\n",
        "!mkdir -p \"$drive_path\"\n",
        "\n",
        "# Copy the saved model to Drive\n",
        "shutil.copy(\"best_rna3d_model.pt\", drive_path + \"best_rna3d_model.pt\")\n",
        "print(f\"Model saved to {drive_path}best_rna3d_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXnvFffDLHHS",
        "outputId": "1d4985df-6374-4471-d84b-09aee2d29cfb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model saved to /content/drive/MyDrive/RNA_3D_Models/best_rna3d_model.pt\n"
          ]
        }
      ]
    }
  ]
}